{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pending-debut",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorboard.summary._tf.summary' has no attribute 'FileWriter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a944cdd44f14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mconfigs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mParseParams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mshared\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbenchmark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbenchmark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Yifei\\Learning\\ReinforcementLearning\\TF4AI_practice\\vrp\\shared\\embeddings.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mconfigs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mParseParams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mshared\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_embedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0museful_files\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgnn_film_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGNN_FiLM_Model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mshared\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_embedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0museful_files\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber_vehicle_task\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNb_Vehicles_Task\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mVRP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvrp_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataGenerator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Yifei\\Learning\\ReinforcementLearning\\TF4AI_practice\\vrp\\shared\\graph_embedding\\useful_files\\gnn_film_model.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0msparse_graph_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparse_Graph_Model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mshared\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_embedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0museful_files\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse_graph_task\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparse_Graph_Task\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mshared\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_embedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0museful_files\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgnn_film\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msparse_gnn_film_layer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msparse_gnn_film_layer_with_dist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Yifei\\Learning\\ReinforcementLearning\\TF4AI_practice\\vrp\\shared\\graph_embedding\\useful_files\\sparse_graph_model.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mSparse_Graph_Model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mABC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \"\"\"\n\u001b[0;32m     18\u001b[0m     \u001b[0mAbstract\u001b[0m \u001b[0msuperclass\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefining\u001b[0m \u001b[0mcore\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mfunctionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Yifei\\Learning\\ReinforcementLearning\\TF4AI_practice\\vrp\\shared\\graph_embedding\\useful_files\\sparse_graph_model.py\u001b[0m in \u001b[0;36mSparse_Graph_Model\u001b[1;34m()\u001b[0m\n\u001b[0;32m    268\u001b[0m                     \u001b[0mdata_fold\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataFold\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m                     \u001b[0mquiet\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 270\u001b[1;33m                     summary_writer: Optional[tf.summary.FileWriter] = None) \\\n\u001b[0m\u001b[0;32m    271\u001b[0m             \u001b[1;33m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m         \"\"\"\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorboard.summary._tf.summary' has no attribute 'FileWriter'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "import shared.misc_utils as utils\n",
    "\n",
    "from configs import ParseParams\n",
    "\n",
    "from shared import embeddings\n",
    "\n",
    "from evaluation.benchmark import benchmark\n",
    "from model.attention_agent import RLAgent\n",
    "\n",
    "def load_task_specific_components(task,ups):\n",
    "    '''\n",
    "    This function load task-specific libraries\n",
    "    '''\n",
    "    if task == 'vrp':\n",
    "        if ups:\n",
    "            from UPS.vrp_ups_utils import DataGenerator,Env,reward_func\n",
    "            from UPS.vrp_ups_attention import AttentionVRP_UPS_Actor, AttentionVRP_UPS_Critic\n",
    "\n",
    "            AttentionActor = AttentionVRP_UPS_Actor\n",
    "            AttentionCritic = AttentionVRP_UPS_Critic\n",
    "\n",
    "        else:\n",
    "            from VRP.vrp_utils import DataGenerator,Env,reward_func\n",
    "            from VRP.vrp_attention import AttentionVRPActor,AttentionVRPCritic\n",
    "\n",
    "            AttentionActor = AttentionVRPActor\n",
    "            AttentionCritic = AttentionVRPCritic\n",
    "\n",
    "    elif task == 'vrptw':\n",
    "        if ups:\n",
    "            from UPS.vrptw_ups_utils import DataGenerator,Env,reward_func\n",
    "            from UPS.vrptw_ups_attention import AttentionVRPTW_UPS_Actor, AttentionVRPTW_UPS_Critic\n",
    "\n",
    "            AttentionActor = AttentionVRPTW_UPS_Actor\n",
    "            AttentionCritic = AttentionVRPTW_UPS_Critic\n",
    "        else:\n",
    "            from VRPTW.vrptw_utils import DataGenerator,Env,reward_func\n",
    "            from VRPTW.vrptw_attention import AttentionVRPTWActor, AttentionVRPTWCritic\n",
    "\n",
    "            AttentionActor = AttentionVRPTWActor\n",
    "            AttentionCritic = AttentionVRPTWCritic\n",
    "\n",
    "    else:\n",
    "        raise Exception('Task is not implemented')\n",
    "\n",
    "    return DataGenerator, Env, reward_func, AttentionActor, AttentionCritic\n",
    "\n",
    "\n",
    "def load_task_specific_eval(task):\n",
    "    \"\"\"\n",
    "    Load taks specific, dependign of tw or not\n",
    "    \"\"\"\n",
    "    if task == 'vrp':\n",
    "        from evaluation.eval_VRP import eval_google_or,eval_Clarke_Wright\n",
    "\n",
    "        return [(eval_google_or.EvalGoogleOR,'or_tools'), (eval_Clarke_Wright.EvalClarkeWright,'Clarke_Wright')]\n",
    "\n",
    "    elif task == 'vrptw':\n",
    "        from evaluation.eval_VRPTW import eval_tw_google_or,eval_I1_heuristics\n",
    "\n",
    "        return [(eval_tw_google_or.EvalTWGoogleOR,'or_tools_tw'),(eval_I1_heuristics.EvalI1Heuristics,'I1_heuristic')]\n",
    "\n",
    "    else:\n",
    "        raise Exception('Task is not implemented')\n",
    "\n",
    "\n",
    "def main(args, prt):\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    \n",
    "    # load task specific classes\n",
    "    DataGenerator, Env, reward_func, AttentionActor, AttentionCritic = \\\n",
    "        load_task_specific_components(args['task_name'],args['ups'])\n",
    "\n",
    "    dataGen = DataGenerator(args)\n",
    "    dataGen.reset()\n",
    "    env = Env(args)\n",
    "    # create an RL agent\n",
    "    agent = RLAgent(args,\n",
    "                    prt,\n",
    "                    env,\n",
    "                    dataGen,\n",
    "                    reward_func,\n",
    "                    AttentionActor,\n",
    "                    AttentionCritic,\n",
    "                    is_train=args['is_train'])\n",
    "    agent.Initialize(sess)\n",
    "\n",
    "    # train or evaluate\n",
    "    prev_actor_loss, prev_critic_loss = float('Inf'), float('Inf')\n",
    "    actor_eps, critic_eps = 1e-2, 1e-2\n",
    "    start_time = time.time()\n",
    "    convergence_counter = 0\n",
    "    al_file = open(args['log_dir']+\"/actorLoss.txt\", \"w\")\n",
    "    cl_file = open(args['log_dir']+\"/criticLoss.txt\", \"w\")\n",
    "    r_file = open(args['log_dir']+\"/reward.txt\", \"w\")\n",
    "\n",
    "    if args['is_train']:\n",
    "        prt.print_out('Training started ...')\n",
    "        train_time_beg = time.time()\n",
    "        for step in range(args['n_train']):\n",
    "            summary = agent.run_train_step()\n",
    "            _, _ , actor_loss_val, critic_loss_val, actor_gra_and_var_val, critic_gra_and_var_val,\\\n",
    "                R_val, v_val, logprobs_val,probs_val, actions_val, idxs_val= summary\n",
    "\n",
    "            curr_actor_loss = np.mean(actor_loss_val)\n",
    "            curr_critic_loss = np.mean(critic_loss_val)\n",
    "            al_file.write( str(actor_loss_val) + '\\n')\n",
    "            cl_file.write(str(critic_loss_val) + '\\n')\n",
    "            r_file.write(str(np.mean(R_val)) + '\\n')\n",
    "\n",
    "            if abs(prev_actor_loss - curr_actor_loss) < actor_eps \\\n",
    "                and abs(prev_critic_loss - curr_critic_loss) < critic_eps:\n",
    "                convergence_counter += 1\n",
    "            else:\n",
    "                convergence_counter = 0\n",
    "            if convergence_counter == 10:\n",
    "                prt.print_out('Converged at step {}'\\\n",
    "                      .format(step))\n",
    "                train_time_end = time.time()-train_time_beg\n",
    "                prt.print_out('Train Step: {} -- Time: {} -- Train reward: {} -- Value: {}'\\\n",
    "                      .format(step,time.strftime(\"%H:%M:%S\", time.gmtime(\\\n",
    "                        train_time_end)),np.mean(R_val),np.mean(v_val)))\n",
    "                prt.print_out('    actor loss: {} -- critic loss: {}'\\\n",
    "                      .format(curr_actor_loss,curr_critic_loss))\n",
    "                break\n",
    "\n",
    "            if step%args['save_interval'] == 0:\n",
    "                agent.saver.save(sess,args['model_dir']+'/model.ckpt', global_step=step)\n",
    "\n",
    "            if step%args['log_interval'] == 0:\n",
    "                train_time_end = time.time()-train_time_beg\n",
    "                prt.print_out('Train Step: {} -- Time: {} -- Embedding Time {} -- Train reward: {} -- Value: {}'\\\n",
    "                      .format(step,time.strftime(\"%H:%M:%S\", time.gmtime(\\\n",
    "                        train_time_end)),time.strftime(\"%H:%M:%S\", time.gmtime(\\\n",
    "                        agent.embedder_model.total_time)),np.mean(R_val),np.mean(v_val)))\n",
    "                prt.print_out('    actor loss: {} -- critic loss: {}'\\\n",
    "                      .format(curr_actor_loss, curr_critic_loss))\n",
    "\n",
    "                train_time_beg = time.time()\n",
    "                agent.embedder_model.total_time = 0\n",
    "            if step%args['test_interval'] == 0:\n",
    "                agent.inference(args['infer_type'])\n",
    "            prev_actor_loss = curr_actor_loss\n",
    "            prev_critic_loss = curr_critic_loss\n",
    "\n",
    "        # Save the model at the end of the training\n",
    "        agent.saver.save(sess,args['model_dir']+'/model.ckpt', global_step=step)\n",
    "\n",
    "    else: # inference\n",
    "        prt.print_out('Evaluation started ...')\n",
    "        agent.inference(args['infer_type'])\n",
    "\n",
    "        all_evaluator = load_task_specific_eval(args['task_name'])\n",
    "\n",
    "        # perform the evaluation\n",
    "        list_eval = ['beam_search'] #['greedy','beam_search']\n",
    "        for eval_tuple in all_evaluator:\n",
    "            list_eval.append(eval_tuple[1])\n",
    "\n",
    "            object_eval = eval_tuple[0](args,env,prt,args['min_trucks'])\n",
    "            object_eval.perform_routing()\n",
    "        #\n",
    "        benchmark_object = benchmark.Benchmark(args,env,prt)\n",
    "        # list_eval.remove('Clarke_Wright')\n",
    "        # #list_eval.remove('I1_heuristic')\n",
    "        benchmark_object.perform_benchmark(list_eval=list_eval)\n",
    "\n",
    "    prt.print_out('Total time is {}'.format(time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_time))))\n",
    "    al_file.close()\n",
    "    cl_file.close()\n",
    "    r_file.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    embeddings.test()\n",
    "    assert False\n",
    "    args, prt = ParseParams()\n",
    "    args['is_train'] = True\n",
    "    # args['infer_type'] = 'single'\n",
    "    args['test_size'] = 1000\n",
    "   #  args['log_dir'] = \"/Users/jpoullet/Documents/MIT/Thesis/ML6867_project/VRP-RL/logs/vrp20-2019-12-05_09-28-11/\"\n",
    "    # args['load_path'] = \"/Users/jpoullet/Documents/MIT/Thesis/ML6867_project/VRP-RL/logs/vrp50-NbTruck/model/\"\n",
    "\n",
    "    # args['data_dir'] = \"drive/My Drive/VRP-RL/data\"\n",
    "    # args['log_dir'] = \"drive/My Drive/VRP-RL/logs\"\n",
    "    # args['log_dir'] = \"{}/{}-{}\".format(args['log_dir'],args['task'], utils.get_time())\n",
    "    # print(args['log_dir'])\n",
    "    # args['model_dir'] = os.path.join(args['log_dir'],'model')\n",
    "    #\n",
    "    # args['load_path'] = \"drive/My Drive/VRP-RL/logs/vrptw50-2019-11-25_01-28-09/model/\"\n",
    "    # print(args['model_dir'])\n",
    "    # # file to write the stdout\n",
    "    # try:\n",
    "    #     os.makedirs(args['log_dir'])\n",
    "    #     os.makedirs(args['model_dir'])\n",
    "    # except:\n",
    "    #     pass\n",
    "    #\n",
    "    # # create a print handler\n",
    "    # out_file = open(os.path.join(args['log_dir'], 'results.txt'),'w+')\n",
    "    # prt = utils.printOut(out_file,args['stdout_print'])\n",
    "\n",
    "    # Random\n",
    "    random_seed = args['random_seed']\n",
    "    if random_seed is not None and random_seed > 0:\n",
    "        prt.print_out(\"# Set random seed to %d\" % random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        tf.set_random_seed(random_seed)\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    main(args, prt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-tuning",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
