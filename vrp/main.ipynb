{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8397ff7-c778-4ce4-944b-7241d780faf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "import shared.misc_utils as utils\n",
    "\n",
    "from configs import ParseParams\n",
    "\n",
    "#from shared import embeddings\n",
    "\n",
    "from evaluation.benchmark import benchmark\n",
    "#from model.attention_agent import RLAgent\n",
    "\n",
    "import pickle,time,os\n",
    "\n",
    "from configs import ParseParams\n",
    "from shared.graph_embedding.useful_files.gnn_film_model import GNN_FiLM_Model\n",
    "from shared.graph_embedding.useful_files.number_vehicle_task import Nb_Vehicles_Task\n",
    "from VRP.vrp_utils import DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80a32e8b-5348-42e7-a271-b888d04b4ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecodeStep(object):\n",
    "    '''\n",
    "    Base class of the decoding (without RNN)\n",
    "    '''\n",
    "    def __init__(self, \n",
    "            ClAttention,\n",
    "            hidden_dim,\n",
    "            use_tanh=False,\n",
    "            tanh_exploration=10.,\n",
    "            n_glimpses=0,\n",
    "            mask_glimpses=True,\n",
    "            mask_pointer=True,\n",
    "            _scope=''):\n",
    "        '''\n",
    "        This class does one-step of decoding.\n",
    "        Inputs:\n",
    "            ClAttention:    the class which is used for attention\n",
    "            hidden_dim:     hidden dimension of RNN\n",
    "            use_tanh:       whether to use tanh exploration or not\n",
    "            tanh_exploration: parameter for tanh exploration\n",
    "            n_glimpses:     number of glimpses\n",
    "            mask_glimpses:  whether to use masking for the glimpses or not\n",
    "            mask_pointer:   whether to use masking for the glimpses or not\n",
    "            _scope:         variable scope\n",
    "        '''\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.use_tanh = use_tanh\n",
    "        self.tanh_exploration = tanh_exploration\n",
    "        self.n_glimpses = n_glimpses\n",
    "        self.mask_glimpses = mask_glimpses\n",
    "        self.mask_pointer = mask_pointer\n",
    "        self._scope = _scope\n",
    "        self.BIGNUMBER = 100000.\n",
    "\n",
    "\n",
    "        # create glimpse and attention instances as well as tf.variables.\n",
    "        ## create a list of class instances\n",
    "        self.glimpses = [None for _ in range(self.n_glimpses)]\n",
    "        for i in range(self.n_glimpses):\n",
    "            self.glimpses[i] = ClAttention(hidden_dim, \n",
    "                use_tanh=False,\n",
    "                _scope=self._scope,\n",
    "                _name=\"Glimpse\"+str(i))\n",
    "            \n",
    "        # build TF variables required for pointer\n",
    "        self.pointer = ClAttention(hidden_dim, \n",
    "            use_tanh=use_tanh, \n",
    "            C=tanh_exploration,\n",
    "            _scope=self._scope,\n",
    "            _name=\"Decoder/Attention\")\n",
    "\n",
    "    def get_logit_op(self,\n",
    "                     decoder_inp,\n",
    "                     context,\n",
    "                     Env,\n",
    "                    *args,\n",
    "                    **kwargs):\n",
    "        \"\"\"\n",
    "        For a given input to deocoder, returns the logit op.\n",
    "        Input:\n",
    "            decoder_inp: it is the input problem with dimensions [batch_size x dim].\n",
    "                        Usually, it is the embedded problem with dim = embedding_dim.\n",
    "            context: the context vetor from the encoder. It is usually the output of rnn with\n",
    "                      shape [batch_size x max_time x dim]\n",
    "            Env: an instance of the environment. It should have:\n",
    "                Env.mask: a matrix used for masking the logits and glimpses. It is with shape\n",
    "                         [batch_size x max_time]. Zeros in this matrix means not-masked nodes. Any \n",
    "                         positive number in this mask means that the node cannot be selected as \n",
    "                         the next decision point.\n",
    "        Returns:\n",
    "            logit: the logits which will used by decoder for producing a solution. It has shape\n",
    "            [batch_size x max_time].\n",
    "        \"\"\"\n",
    "\n",
    "        # glimpses\n",
    "        for i in range(self.n_glimpses):\n",
    "            # ref: [batch_size x max_time x hidden_dim], logit : [batch_size x max_time]\n",
    "            ref, logit = self.glimpses[i](decoder_inp, context,Env)\n",
    "            if self.mask_glimpses:\n",
    "                logit -= self.BIGNUMBER* Env.mask\n",
    "            # prob: [batch_size x max_time\n",
    "            prob = tf.nn.softmax(logit)\n",
    "            # decoder_inp : [batch_size x 1 x max_time ] * [batch_size x max_time x hidden_dim] -> \n",
    "            #[batch_size x hidden_dim ]\n",
    "            decoder_inp = tf.squeeze(tf.matmul( tf.expand_dims(prob,1),ref) ,1)\n",
    "\n",
    "        # attention\n",
    "        _, logit = self.pointer(decoder_inp,context,Env)\n",
    "        if self.mask_pointer:\n",
    "            logit -= self.BIGNUMBER* Env.mask\n",
    "\n",
    "        return logit , None\n",
    "\n",
    "    def step(self,\n",
    "            decoder_inp,\n",
    "            context,\n",
    "            Env,\n",
    "            decoder_state=None,\n",
    "            *args,\n",
    "            **kwargs):\n",
    "        '''\n",
    "        get logits and probs at a given decoding step.\n",
    "        Inputs:\n",
    "            decoder_input: Input of the decoding step with shape [batch_size x embedding_dim]\n",
    "            context: context vector to use in attention\n",
    "            Env: an instance of the environment\n",
    "            decoder_state: The state of the LSTM cell. It can be None when we use a decoder without \n",
    "                LSTM cell.\n",
    "        Returns:\n",
    "            logit: logits with shape [batch_size x max_time]\n",
    "            prob: probabilities for the next location visit with shape of [batch_size x max_time]\n",
    "            logprob: log of probabilities\n",
    "            decoder_state: updated state of the LSTM cell\n",
    "        '''\n",
    "\n",
    "        logit, decoder_state = self.get_logit_op(\n",
    "                     decoder_inp,\n",
    "                     context,\n",
    "                     Env, \n",
    "                     decoder_state)\n",
    "\n",
    "        logprob = tf.nn.log_softmax(logit)\n",
    "        prob = tf.exp(logprob)\n",
    "        #prob has shape(batch_num, node_number ),  here is (batch_number, 11),  for each batch,  the one node's value will be close to 1, which\n",
    "        #is the stop this agent believe should visit next\n",
    "        #next step,  figure out what to do next with this prob\n",
    "        return logit, prob, logprob, decoder_state\n",
    "\n",
    "class RNNDecodeStep(DecodeStep):\n",
    "    '''\n",
    "    Decodes the sequence. It keeps the decoding history in a RNN.\n",
    "    '''\n",
    "    def __init__(self, \n",
    "            ClAttention,\n",
    "            hidden_dim,\n",
    "            use_tanh=False,\n",
    "            tanh_exploration=10.,\n",
    "            n_glimpses=0,\n",
    "            mask_glimpses=True,\n",
    "            mask_pointer=True,\n",
    "            forget_bias=1.0,\n",
    "            rnn_layers=1,\n",
    "            _scope=''):\n",
    "\n",
    "        '''\n",
    "        This class does one-step of decoding which uses RNN for storing the sequence info.\n",
    "        Inputs:\n",
    "            ClAttention:    the class which is used for attention\n",
    "            hidden_dim:     hidden dimension of RNN\n",
    "            use_tanh:       whether to use tanh exploration or not\n",
    "            tanh_exploration: parameter for tanh exploration\n",
    "            n_glimpses:     number of glimpses\n",
    "            mask_glimpses:  whether to use masking for the glimpses or not\n",
    "            mask_pointer:   whether to use masking for the glimpses or not\n",
    "            forget_bias:    forget bias of LSTM\n",
    "            rnn_layers:     number of LSTM layers\n",
    "            _scope:         variable scope\n",
    "\n",
    "        '''\n",
    "\n",
    "        super(RNNDecodeStep,self).__init__(ClAttention,\n",
    "                                        hidden_dim,\n",
    "                                        use_tanh=use_tanh,\n",
    "                                        tanh_exploration=tanh_exploration,\n",
    "                                        n_glimpses=n_glimpses,\n",
    "                                        mask_glimpses=mask_glimpses,\n",
    "                                        mask_pointer=mask_pointer,\n",
    "                                        _scope=_scope)\n",
    "        self.forget_bias = forget_bias\n",
    "        self.rnn_layers = rnn_layers     \n",
    "#         self.dropout = tf.placeholder(tf.float32,name='decoder_rnn_dropout')\n",
    "\n",
    "        # build a multilayer LSTM cell\n",
    "        single_cell = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(hidden_dim, \n",
    "            forget_bias=forget_bias)\n",
    "        self.dropout = tf.compat.v1.placeholder(tf.float32,name='decoder_rnn_dropout') \n",
    "        single_cell = tf.compat.v1.nn.rnn_cell.DropoutWrapper(\n",
    "                cell=single_cell, input_keep_prob=(1.0 - self.dropout))\n",
    "        self.cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell([single_cell] * rnn_layers)\n",
    "\n",
    "    def get_logit_op(self,\n",
    "                    decoder_inp,\n",
    "                    context,\n",
    "                    Env,\n",
    "                    decoder_state,\n",
    "                    *args,\n",
    "                    **kwargs):\n",
    "        \"\"\"\n",
    "        For a given input to decoder, returns the logit op and new decoder_state.\n",
    "        Input:\n",
    "            decoder_inp: it is the input problem with dimensions [batch_size x dim].\n",
    "                        Usually, it is the embedded problem with dim = embedding_dim.\n",
    "            context: the context vetor from the encoder. It is usually the output of rnn with\n",
    "                      shape [batch_size x max_time x dim]\n",
    "            Env: an instance of the environment. It should have:\n",
    "                Env.mask: a matrix used for masking the logits and glimpses. It is with shape\n",
    "                         [batch_size x max_time]. Zeros in this matrix means not-masked nodes. Any \n",
    "                         positive number in this mask means that the node cannot be selected as \n",
    "                         the next decision point.\n",
    "            decoder_state: The state as a list of size rnn_layers, and each element is a\n",
    "                    LSTMStateTuples with  x 2 tensors with dimension of [batch_size x hidden_dim].\n",
    "                    The first one corresponds to c and the second one is h.\n",
    "        Returns:\n",
    "            logit: the logits which will used by decoder for producing a solution. It has shape\n",
    "                    [batch_size x max_time].\n",
    "            decoder_state: the update decoder state.\n",
    "        \"\"\"\n",
    "\n",
    "        # decoder_inp = tf.reshape(decoder_inp,[-1,1,self.hidden_dim])\n",
    "        _ , decoder_state = tf.compat.v1.nn.dynamic_rnn(self.cell,\n",
    "                                              decoder_inp,\n",
    "                                              initial_state=decoder_state,\n",
    "                                              scope=self._scope+'Decoder/LSTM/rnn')\n",
    "        hy = decoder_state[-1].h\n",
    "\n",
    "        # glimpses\n",
    "        for i in range(self.n_glimpses):\n",
    "            # ref: [batch_size x max_time x hidden_dim], logit : [batch_size x max_time]\n",
    "            ref, logit = self.glimpses[i](hy,context,Env)\n",
    "            if self.mask_glimpses:\n",
    "                logit -= self.BIGNUMBER* Env.mask\n",
    "            prob = tf.nn.softmax(logit)\n",
    "            \n",
    "            # hy : [batch_size x 1 x max_time ] * [batch_size x max_time x hidden_dim] -> \n",
    "            #[batch_size x hidden_dim ]\n",
    "            hy = tf.squeeze(tf.matmul( tf.expand_dims(prob,1),ref) ,1)\n",
    "\n",
    "        # attention\n",
    "        _, logit = self.pointer(hy,context,Env)\n",
    "        if self.mask_pointer:\n",
    "            logit -= self.BIGNUMBER* Env.mask\n",
    "    \n",
    "        return logit , decoder_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4674f0d0-1726-4a87-b42e-666466656f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time, os\n",
    "\n",
    "from shutil import copyfile\n",
    "from sklearn.preprocessing import normalize\n",
    "from shared.embeddings import LinearEmbedding,GraphEmbedding\n",
    "from shared.graph_embedding.full_graph_learning import FullGraphEmbedding\n",
    "#from shared.decode_step import RNNDecodeStep\n",
    "\n",
    "class RLAgent(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                args,\n",
    "                prt,\n",
    "                env,\n",
    "                dataGen,\n",
    "                reward_func,\n",
    "                clAttentionActor,\n",
    "                clAttentionCritic,\n",
    "                is_train=True,\n",
    "                _scope=''):\n",
    "        '''\n",
    "        This class builds the model and run testt and train.\n",
    "        Inputs:\n",
    "            args: arguments. See the description in config.py file.\n",
    "            prt: print controller which writes logs to a file.\n",
    "            env: an instance of the environment.\n",
    "            dataGen: a data generator which generates data for test and training.\n",
    "            reward_func: the function which is used for computing the reward. It returns the tour length.\n",
    "            clAttentionActor: Attention mechanism that is used in actor.\n",
    "            clAttentionCritic: Attention mechanism that is used in critic.\n",
    "            is_train: if true, the agent is used for training; else, it is used only\n",
    "                        for inference.\n",
    "        '''\n",
    "\n",
    "        self.args = args\n",
    "        self.prt = prt\n",
    "        self.env = env\n",
    "        self.dataGen = dataGen\n",
    "        self.reward_func = reward_func\n",
    "        self.clAttentionCritic = clAttentionCritic\n",
    "\n",
    "        if args['embedding_graph'] == 2:\n",
    "            self.embedder_model = FullGraphEmbedding(args['embedding_dim'],args)\n",
    "        else:\n",
    "            self.embedder_model = LinearEmbedding(args['embedding_dim'],_scope=_scope+'Actor/')\n",
    "\n",
    "        if args['embedding_graph'] ==1:\n",
    "            data_test = self.dataGen.get_test_all()\n",
    "            self.embedder_graph = GraphEmbedding(args,data_test)\n",
    "\n",
    "\n",
    "\n",
    "        self.decodeStep = RNNDecodeStep(clAttentionActor,\n",
    "                        args['hidden_dim'],\n",
    "                        use_tanh=args['use_tanh'],\n",
    "                        tanh_exploration=args['tanh_exploration'],\n",
    "                        n_glimpses=args['n_glimpses'],\n",
    "                        mask_glimpses=args['mask_glimpses'],\n",
    "                        mask_pointer=args['mask_pointer'],\n",
    "                        forget_bias=args['forget_bias'],\n",
    "                        rnn_layers=args['rnn_layers'],\n",
    "                        _scope='Actor/')\n",
    "        self.decoder_input = tf.compat.v1.get_variable('decoder_input', [1,1,args['embedding_dim']],\n",
    "                       initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "\n",
    "        start_time  = time.time()\n",
    "        if is_train:\n",
    "            self.train_summary = self.build_model(decode_type = \"stochastic\" )\n",
    "            self.train_step = self.build_train_step()\n",
    "\n",
    "        self.val_summary_greedy = self.build_model(decode_type = \"greedy\" )\n",
    "        self.val_summary_beam = self.build_model(decode_type = \"beam_search\")\n",
    "\n",
    "        model_time = time.time()- start_time\n",
    "        self.prt.print_out(\"It took {}s to build the agent.\".format(str(model_time)))\n",
    "\n",
    "        self.saver = tf.compat.v1.train.Saver(\n",
    "            var_list=tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES))\n",
    "\n",
    "        self.out_avg_resul = open(args['log_dir']+\"/avg_inference.txt\", \"w\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "725177cd-c5ad-461e-82cf-f25ec5645301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from shared.graph_embedding.useful_files.utils import get_activation, get_aggregation_function, SMALL_NUMBER\n",
    "\n",
    "def layer_norm(input_tensor, name=None):\n",
    "  \"\"\"Run layer normalization on the last dimension of the tensor.\"\"\"\n",
    "  return tf.keras.layers.LayerNormalization(name=name,axis=-1,epsilon=1e-12,dtype=tf.float32)(input_tensor)\n",
    "\n",
    "def sparse_gnn_film_layer(node_embeddings: tf.Tensor,\n",
    "                          adjacency_lists: List[tf.Tensor],\n",
    "                          type_to_num_incoming_edges: tf.Tensor,\n",
    "                          state_dim: Optional[int],\n",
    "                          num_timesteps: int = 1,\n",
    "                          activation_function: Optional[str] = \"ReLU\",\n",
    "                          message_aggregation_function: str = \"sum\",\n",
    "                          normalize_by_num_incoming: bool = False,\n",
    "                          ) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Compute new graph states by neural message passing modulated by the target state.\n",
    "    For this, we assume existing node states h^t_v and a list of per-edge-type adjacency\n",
    "    matrices A_\\ell.\n",
    "\n",
    "    We compute new states as follows:\n",
    "        h^{t+1}_v := \\sum_\\ell\n",
    "                     \\sum_{(u, v) \\in A_\\ell}\n",
    "                        \\sigma(1/c_{v,\\ell} * \\alpha_{\\ell,v} * (W_\\ell * h^t_u) + \\beta_{\\ell,v})\n",
    "        \\alpha_{\\ell,v} := F_{\\ell,\\alpha} * h^t_v\n",
    "        \\beta_{\\ell,v} := F_{\\ell,\\beta} * h^t_v\n",
    "        c_{\\v,\\ell} is usually 1 (but could also be the number of incoming edges).\n",
    "    The learnable parameters of this are the W_\\ell, F_{\\ell,\\alpha}, F_{\\ell,\\beta} \\in R^{D, D}.\n",
    "\n",
    "    We use the following abbreviations in shape descriptions:\n",
    "    * V: number of nodes\n",
    "    * D: state dimension\n",
    "    * L: number of different edge types\n",
    "    * E: number of edges of a given edge type\n",
    "\n",
    "    Arguments:\n",
    "        node_embeddings: float32 tensor of shape [V, D], the original representation of\n",
    "            each node in the graph.\n",
    "        adjacency_lists: List of L adjacency lists, represented as int32 tensors of shape\n",
    "            [E, 2]. Concretely, adjacency_lists[l][k,:] == [v, u] means that the k-th edge\n",
    "            of type l connects node v to node u.\n",
    "        type_to_num_incoming_edges: float32 tensor of shape [L, V] representing the number\n",
    "            of incoming edges of a given type. Concretely, type_to_num_incoming_edges[l, v]\n",
    "            is the number of edge of type l connecting to node v.\n",
    "        state_dim: Optional size of output dimension of the GNN layer. If not set, defaults\n",
    "            to D, the dimensionality of the input. If different from the input dimension,\n",
    "            parameter num_timesteps has to be 1.\n",
    "        num_timesteps: Number of repeated applications of this message passing layer.\n",
    "        activation_function: Type of activation function used.\n",
    "        message_aggregation_function: Type of aggregation function used for messages.\n",
    "        normalize_by_num_incoming: Flag indicating if messages should be scaled by 1/(number\n",
    "            of incoming edges).\n",
    "\n",
    "    Returns:\n",
    "        float32 tensor of shape [V, state_dim]\n",
    "    \"\"\"\n",
    "    num_nodes = tf.shape(input=node_embeddings, out_type=tf.int32)[0]\n",
    "    if state_dim is None:\n",
    "        state_dim = tf.shape(input=node_embeddings, out_type=tf.int32)[1]\n",
    "\n",
    "    # === Prepare things we need across all timesteps:\n",
    "    activation_fn = get_activation(activation_function)\n",
    "    message_aggregation_fn = get_aggregation_function(message_aggregation_function)\n",
    "    edge_type_to_message_transformation_layers = []  # Layers to compute the message from a source state\n",
    "    edge_type_to_film_computation_layers = []  # Layers to compute the \\beta/\\gamma weights for FiLM\n",
    "    edge_type_to_message_targets = []  # List of tensors of message targets\n",
    "    for edge_type_idx, adjacency_list_for_edge_type in enumerate(adjacency_lists):\n",
    "        edge_type_to_message_transformation_layers.append(\n",
    "            tf.keras.layers.Dense(units=state_dim,\n",
    "                                  use_bias=False,\n",
    "                                  activation=None,  # Activation only after FiLM modulation\n",
    "                                  name=\"Edge_%i_Weight\" % edge_type_idx))\n",
    "        edge_type_to_film_computation_layers.append(\n",
    "            tf.keras.layers.Dense(units=2 * state_dim,  # Computes \\gamma, \\beta in one go\n",
    "                                  use_bias=False,\n",
    "                                  activation=None,\n",
    "                                  name=\"Edge_%i_FiLM_Computations\" % edge_type_idx))\n",
    "        edge_type_to_message_targets.append(tf.cast(adjacency_list_for_edge_type[:, 1],dtype=tf.int32))\n",
    "\n",
    "    # Let M be the number of messages (sum of all E):\n",
    "    message_targets = tf.concat(edge_type_to_message_targets, axis=0)  # Shape [M]\n",
    "\n",
    "    cur_node_states = node_embeddings\n",
    "    for _ in range(num_timesteps):\n",
    "        messages_per_type = []  # list of tensors of messages of shape [E, D]\n",
    "        # Collect incoming messages per edge type\n",
    "        for edge_type_idx, adjacency_list_for_edge_type in enumerate(adjacency_lists):\n",
    "            edge_sources = tf.cast(adjacency_list_for_edge_type[:, 0],dtype=tf.int32)\n",
    "            edge_targets = tf.cast(adjacency_list_for_edge_type[:, 1],dtype=tf.int32)\n",
    "            edge_source_states = \\\n",
    "                tf.nn.embedding_lookup(params=cur_node_states,\n",
    "                                       ids=edge_sources)  # Shape [E, D]\n",
    "            #embedding_lookup:  if params is a tensor like [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]], \n",
    "            #and ids is: [0, 3, 4],  then return value would be [[1, 2], [7, 8], [9, 10]]\n",
    "            messages = edge_type_to_message_transformation_layers[edge_type_idx](edge_source_states)  # Shape [E, D]\n",
    "\n",
    "            if normalize_by_num_incoming:\n",
    "                per_message_num_incoming_edges = \\\n",
    "                    tf.nn.embedding_lookup(params=type_to_num_incoming_edges[edge_type_idx, :],\n",
    "                                           ids=edge_targets)  # Shape [E, H]\n",
    "\n",
    "                messages = tf.expand_dims(1.0 / (per_message_num_incoming_edges + SMALL_NUMBER), axis=-1) * messages\n",
    "\n",
    "            film_weights = edge_type_to_film_computation_layers[edge_type_idx](cur_node_states)\n",
    "            per_message_film_weights = \\\n",
    "                tf.nn.embedding_lookup(params=film_weights, ids=edge_targets)\n",
    "            per_message_film_gamma_weights = per_message_film_weights[:, :state_dim]  # Shape [E, D]\n",
    "            per_message_film_beta_weights = per_message_film_weights[:, state_dim:]  # Shape [E, D]\n",
    "\n",
    "            modulated_messages = per_message_film_gamma_weights * messages + per_message_film_beta_weights\n",
    "            messages_per_type.append(modulated_messages)\n",
    "\n",
    "        all_messages = tf.concat(messages_per_type, axis=0)  # Shape [M, D]\n",
    "        all_messages = activation_fn(all_messages)  # Shape [M, D]\n",
    "        aggregated_messages = \\\n",
    "            message_aggregation_fn(data=all_messages,\n",
    "                                   segment_ids=message_targets,\n",
    "                                   num_segments=num_nodes)  # Shape [V, D]\n",
    "        new_node_states = aggregated_messages\n",
    "        # new_node_states = activation_fn(new_node_states)\n",
    "\n",
    "        cur_node_states = layer_norm(new_node_states)\n",
    "\n",
    "    return cur_node_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "491cb3f3-c67c-4602-bc19-3480dc5e845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "from shared.embeddings import Embedding\n",
    "from shared.graph_embedding.useful_files.utils import get_activation\n",
    "#from shared.graph_embedding.useful_files.gnn_film import sparse_gnn_film_layer\n",
    "\n",
    "class FullGraphEmbedding(Embedding):\n",
    "    \"\"\"\n",
    "    Implements a graph embedding, not test\n",
    "    \"\"\"\n",
    "    def __init__(self,embedding_dim,args):\n",
    "        assert args['embedding_dim'] == 30, args['embedding_dim']\n",
    "        super(FullGraphEmbedding,self).__init__('full_graph',embedding_dim)\n",
    "\n",
    "        self.nb_feat = args['input_dim']\n",
    "        self.n_nodes = args['n_nodes']\n",
    "\n",
    "        self._scale = [5,12,25,50,100]\n",
    "        self._scale = [i * np.sqrt(2)/100 for i in self._scale]     # rescale to the square\n",
    "\n",
    "        self.drop_out = tf.compat.v1.placeholder(tf.float32,name='embedder_graph_dropout')\n",
    "        self.params = {\n",
    "            'graph_num_layers': 8,\n",
    "            'graph_num_timesteps_per_layer': 3,\n",
    "\n",
    "            'graph_layer_input_dropout_keep_prob': 0.8,\n",
    "            'graph_dense_between_every_num_gnn_layers': 1,\n",
    "            'graph_model_activation_function': 'tanh',\n",
    "            'graph_residual_connection_every_num_layers': 1,\n",
    "            'graph_inter_layer_norm': False,\n",
    "            \"hidden_size\": 30,\n",
    "            \"graph_activation_function\": \"ReLU\",\n",
    "            \"message_aggregation_function\": \"sum\",\n",
    "            \"normalize_messages_by_num_incoming\": True\n",
    "            }\n",
    "\n",
    "\n",
    "    def _propagate_graph_model(self,initial_node_features, incoming_edge, list_pair_adjancy):\n",
    "        \"\"\"\n",
    "        Build the propagation model via graph\n",
    "        :param initial_node_features:\n",
    "        :param incoming_edge:\n",
    "        :param list_pair_adjancy:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        h_dim= self.params['hidden_size']\n",
    "        activation_fn = get_activation(self.params['graph_model_activation_function'])\n",
    "\n",
    "        projected_node_features = tf.keras.layers.Dense(units=h_dim,\n",
    "                                      use_bias=False,\n",
    "                                      activation=activation_fn,\n",
    "                                      )(initial_node_features)\n",
    "\n",
    "        cur_node_representations = projected_node_features\n",
    "        last_residual_representations = tf.zeros_like(cur_node_representations)\n",
    "        for layer_idx in range(self.params['graph_num_layers']):\n",
    "            # with tf.variable_scope('gnn_layer_%i' % layer_idx):\n",
    "            cur_node_representations = \\\n",
    "                tf.nn.dropout(cur_node_representations, rate= 1- self.drop_out)\n",
    "            if layer_idx % self.params['graph_residual_connection_every_num_layers'] == 0:\n",
    "                t = cur_node_representations\n",
    "                if layer_idx > 0:\n",
    "                    cur_node_representations += last_residual_representations\n",
    "                    cur_node_representations /= 2\n",
    "                last_residual_representations = t\n",
    "            cur_node_representations = \\\n",
    "                self._apply_gnn_layer(cur_node_representations,list_pair_adjancy,incoming_edge,self.params['graph_num_timesteps_per_layer'])\n",
    "            if self.params['graph_inter_layer_norm']:\n",
    "                cur_node_representations = tf.contrib.layers.layer_norm(cur_node_representations)\n",
    "            if layer_idx % self.params['graph_dense_between_every_num_gnn_layers'] == 0:\n",
    "                cur_node_representations = \\\n",
    "                    tf.keras.layers.Dense(units=h_dim,\n",
    "                                          use_bias=False,\n",
    "                                          activation=activation_fn,\n",
    "                                          name=\"Dense\",\n",
    "                                          )(cur_node_representations)\n",
    "\n",
    "        return cur_node_representations\n",
    "\n",
    "\n",
    "    def _apply_gnn_layer(self,node_representations,adjacency_lists,type_to_num_incoming_edges,num_timesteps):\n",
    "        \"\"\"\n",
    "        Apply the actual gnn layer\n",
    "        \"\"\"\n",
    "        return sparse_gnn_film_layer(\n",
    "            node_embeddings=node_representations,\n",
    "            adjacency_lists=adjacency_lists,\n",
    "            type_to_num_incoming_edges=type_to_num_incoming_edges,\n",
    "            state_dim=self.params['hidden_size'],\n",
    "            num_timesteps=num_timesteps,\n",
    "            activation_function=self.params['graph_activation_function'],\n",
    "            message_aggregation_function=self.params['message_aggregation_function'],\n",
    "            normalize_by_num_incoming=self.params[\"normalize_messages_by_num_incoming\"])\n",
    "\n",
    "\n",
    "    def _prepare_input_data(self, input_tf):\n",
    "        \"\"\"\n",
    "        Prepare the input data so that they are at the right size\n",
    "        :param input_tf:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        #shape of input_tf is [None, 11, 3] which mean undetermined batches, 11 nodes,  andd 3 columne for each node to list the x, y coordinate and demand qty\n",
    "        batch_features = tf.reshape(input_tf,[-1,self.nb_feat])\n",
    "        #batch features are put the nodes infor together,  into shape [None, 3]\n",
    "        input_dist = input_tf[:,:,:2]\n",
    "        square_input = tf.reduce_sum(input_tensor=tf.square(input_dist), axis=2)\n",
    "        row = tf.reshape(square_input, [-1,self.n_nodes,1])\n",
    "        col= tf.reshape(square_input,[-1,1,self.n_nodes])\n",
    "        dist_matrix = tf.sqrt(tf.maximum(row - 2 * tf.matmul(input_dist,input_dist,False,True) + col,0.0))\n",
    "        #shape of dist_matrix would be [?, self.n_nodes, self.n_nodes]\n",
    "        # value is the distance between nodes,  coordinate of node Ni is (Xi1, Xi2), and node Nj is (Xj1, Xj2)\n",
    "        #then the value in the maxtrix for position[ ?, i, j] would be sqrt( (Xi1 - Xj1 ) ^2 + (Xi2 - Xj2)^2 )\n",
    "        # example dist_matrix: \n",
    "        # dist_matrix: \n",
    "        # tf.Tensor(\n",
    "        #[[[ 0.         4.2426405  8.485281  12.7279215]\n",
    "        #  [ 4.2426405  0.         4.2426405  8.485281 ]\n",
    "        #  [ 8.485281   4.2426405  0.         4.2426405]\n",
    "        #  [12.7279215  8.485281   4.2426405  0.       ]]\n",
    "        # [[ 0.         4.2426405  8.485281  12.7279215]\n",
    "        #  [ 4.2426405  0.         4.2426405  8.485281 ]\n",
    "        #  [ 8.485281   4.2426405  0.         4.2426405]\n",
    "        #  [12.7279215  8.485281   4.2426405  0.       ]]], shape=(2, 4, 4), \n",
    "        list_num_incoming_ege = []\n",
    "        list_pair_edge = []\n",
    "        # not_masked is a [?, self.n_nodes, self.n_nodes] shape boolean tensor, all intial values are true\n",
    "        not_masked = tf.ones_like(dist_matrix,dtype=tf.bool)\n",
    "        temp = tf.zeros_like(not_masked[0,:,:])\n",
    "        #so set_diag will set the diagnal values to zeros, like:\n",
    "        #  [ [0, 1, 1, 1],\n",
    "        #    [1, 0, 1, 1],\n",
    "        #    [1, 1, 0, 1],\n",
    "        diag = tf.zeros_like(not_masked[0,:,:])\n",
    "        #tf.linalg.set_diag(not_masked,tf.zeros_like(not_masked[0,:,:])) #linalg_.set_diag won't change not_masked here,  shall change it to not_masked = tf.linalg.set_diag?\n",
    "        #and shape of not_masked[0,:,:] is not correct\n",
    "        not_masked = tf.linalg.set_diag(not_masked,tf.zeros_like(not_masked[:,:,0]))\n",
    "        for i in range(len(self._scale)):\n",
    "            true_for_edge = tf.less_equal(dist_matrix,self._scale[i])\n",
    "            true_for_edge = tf.logical_and(not_masked,true_for_edge)\n",
    "            # continue above example , \n",
    "            # true for edge less than 5.0: \n",
    "            #tf.Tensor(\n",
    "            #[[[ True,  True, False],\n",
    "            #  [ False,  True,  True]],\n",
    "            # [[ True,  True, False],\n",
    "            # [ True,  False,  True]]], shape=(2, 2, 3), dtype=bool)\n",
    "            \n",
    "            # all values less than self._scale[i] are true,  and diagnal values are false\n",
    "            # same shape as dist_matrix [?,n,n]\n",
    "            indices = tf.cast(tf.compat.v1.where(true_for_edge),dtype=tf.int32)\n",
    "            #indices of  coordinates of all the edges in dist_matrix with value less than self._scale[i]\n",
    "            # indices is like:  [[0, 0, 0],\n",
    "            #[0, 0, 1],\n",
    "            #[0, 1, 1],\n",
    "            #[0, 1, 2],\n",
    "            #[1, 0, 0],\n",
    "            #[1, 0, 1],\n",
    "            #[1, 1, 0],\n",
    "            #[1, 1, 2]]\n",
    "            #  notice that the batch dimension is removed,  the following steps will add offset for batch to the index\n",
    "            offset = self.n_nodes * indices[:,0]    # indices' shape is [8,3], so this line will get all batch value\n",
    "             # offset of temp would be [0,0,0,0,11,11,11,11]\n",
    "            offset = tf.expand_dims(offset,axis=1)\n",
    "            #after expending become: [[0],[0],[0],[0],[11],[11], [11],[11]]\n",
    "            offset = tf.tile(offset,[1,2])\n",
    "            #after tiline become:  [[0,0], [0,0], [11,11],[22,22],[22,22],[22,22]]\n",
    "            #indices[:,1:3] is the index of last two column,  so really the edge (from node to 'to node')\n",
    "            true_indices_nodes = offset + indices[:,1:3]\n",
    "            # so now actually true_indices_nodes is like embedding the batching dimension into the edge columns\n",
    "            #[[ 0,  0],\n",
    "           #[ 0,  1],\n",
    "           #[ 1,  1],\n",
    "           #[ 1,  2],\n",
    "           #[11, 11],\n",
    "           #[11, 12],\n",
    "           #[12, 11],\n",
    "           #[12, 13]]\n",
    "            list_pair_edge.append(true_indices_nodes)\n",
    "\n",
    "            num_incoming = tf.reduce_sum(input_tensor=tf.cast(true_for_edge,dtype=tf.int32), axis=1)\n",
    "            # continue examplesï¼Œ num incoming, incoming edges to a node which is less than scale[i]\n",
    "            #tf.Tensor(\n",
    "            #[[1, 2, 1],\n",
    "            #[2, 1, 1]], shape=(2, 3), dtype=int32)\n",
    "            \n",
    "            num_incoming = tf.squeeze(tf.reshape(num_incoming,[1,-1]),0)\n",
    "            # reshaped: tf.Tensor([[1,2,1,2,1,1]], shape=(1, 6), dtype=int32)\n",
    "            # squeezed: tf.Tensor([1,2,1,2,1,1], shape=(6,), dtype=int32)  \n",
    "            list_num_incoming_ege.append(tf.cast(num_incoming,dtype=tf.float32))\n",
    "            # list_num_incoming_ege is a list of 5 tensor\n",
    "            # update the mask\n",
    "            not_masked = tf.logical_and(not_masked,tf.logical_not(true_for_edge)) # we update the mask. The only one not masked are the one wich\n",
    "                                                                                    # were not and did not belong to the edge type\n",
    "        final_incoming_edge = tf.stack(list_num_incoming_ege)   #list_num_incoming_ege is a list of 5 tensors, tensor shape for examples are all (6,),\n",
    "                                                                #  after stack, the will get a tensor with shape (5,6)\n",
    "\n",
    "\n",
    "        return batch_features, final_incoming_edge, list_pair_edge\n",
    "\n",
    "    def __call__(self, input_tf):\n",
    "        \"\"\"\n",
    "        return the node embedding\n",
    "        :param input_tf: the tensor corresponding to the embedding\n",
    "        :return: a tensor\n",
    "        \"\"\"\n",
    "        time_init = time.time()\n",
    "        initial_node_features, incoming_edge, list_pair_adjancy = self._prepare_input_data(input_tf)\n",
    "\n",
    "        final_node_representations = self._propagate_graph_model(initial_node_features,incoming_edge,list_pair_adjancy)\n",
    "        final_node_representations = tf.reshape(final_node_representations,[-1,self.n_nodes,self.embedding_dim])\n",
    "\n",
    "        self.total_time += time.time() - time_init\n",
    "\n",
    "        return final_node_representations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "673eb1d0-67bb-422f-b563-a9e320631646",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAgent( RLAgent ):\n",
    "    def build_model(self, decode_type = \"greedy\"):\n",
    "\n",
    "        # builds the model\n",
    "        args = self.args\n",
    "        env = self.env\n",
    "        batch_size = tf.shape(input=env.input_pnt)[0]\n",
    "\n",
    "        # input_pnt: [batch_size x max_time x dim_task]\n",
    "        input_pnt = env.input_pnt\n",
    "\n",
    "        # encoder_emb_inp: [batch_size, max_time, embedding_dim]\n",
    "        if self.args['embedding_graph'] == 0:\n",
    "            encoder_emb_inp = self.embedder_model(input_pnt)\n",
    "        elif self.args['embedding_graph'] == 1:\n",
    "            encoder_emb_inp = self.env.embeded_data\n",
    "        else:\n",
    "            encoder_emb_inp = self.embedder_model(env.input_data_norm)\n",
    "\n",
    "        if decode_type == 'greedy' or decode_type == 'stochastic':\n",
    "            beam_width = 1\n",
    "        elif decode_type == 'beam_search':\n",
    "            beam_width = args['beam_width']\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        # reset the env. The environment is modified to handle beam_search decoding.\n",
    "        env.reset(beam_width)\n",
    "\n",
    "        BatchSequence = tf.expand_dims(tf.cast(tf.range(batch_size*beam_width), tf.int64), 1)\n",
    "        # BatchSequence would be [[0],[1], [2],...[batch_size*beam_width -1]]\n",
    "\n",
    "        # create tensors and lists\n",
    "        actions_tmp = []\n",
    "        logprobs = []\n",
    "        probs = []\n",
    "        idxs = []\n",
    "\n",
    "        # start from depot\n",
    "        idx = (env.n_nodes-1)*tf.ones([batch_size*beam_width,1])\n",
    "        tmp = input_pnt[:,env.n_nodes-1] #temp will get [x,y] coordinate of the depot for each instance\n",
    "        action = tf.tile(tmp,[beam_width,1])  #with beam_with=1,  action is the same as tmp\n",
    "\n",
    "        # decoder_state\n",
    "        initial_state = tf.zeros([args['rnn_layers'], 2, batch_size*beam_width, args['hidden_dim']])\n",
    "        l = tf.unstack(initial_state, axis=0)  #l is a list of tensors\n",
    "        decoder_state = tuple([tf.compat.v1.nn.rnn_cell.LSTMStateTuple(l[idx][0],l[idx][1])\n",
    "                  for idx in range(args['rnn_layers'])])\n",
    "\n",
    "        # start from depot in VRP\n",
    "        # decoder_input: [batch_size*beam_width x 1 x hidden_dim]\n",
    "        decoder_input = tf.tile(tf.expand_dims(encoder_emb_inp[:,env.n_nodes-1], 1),\n",
    "                                [beam_width,1,1])\n",
    "\n",
    "        # decoding loop\n",
    "        context = tf.tile(encoder_emb_inp,[beam_width,1,1])\n",
    "        for i in range(args['decode_len']):  # decode_len is 20\n",
    "\n",
    "            logit, prob, logprob, decoder_state = self.decodeStep.step(decoder_input,\n",
    "                                context,\n",
    "                                env,\n",
    "                                decoder_state)\n",
    "            # idx: [batch_size*beam_width x 1]\n",
    "            beam_parent = None\n",
    "            if decode_type == 'greedy':\n",
    "                idx = tf.expand_dims(tf.argmax(input=prob, axis=1),1)\n",
    "                #idx is the index of node each batch should visit at this step,  its like this: [[2],[10],[0].....]\n",
    "            elif decode_type == 'stochastic':\n",
    "                # select stochastic actions. idx has shape [batch_size x 1]\n",
    "                # tf.multinomial sometimes gives numerical errors, so we use our multinomial :(\n",
    "                def my_multinomial():\n",
    "                    prob_idx = tf.stop_gradient(prob)\n",
    "                    prob_idx_cum = tf.cumsum(prob_idx,1)\n",
    "                    rand_uni = tf.tile(tf.random.uniform([batch_size,1]),[1,env.n_nodes])\n",
    "                    # sorted_ind : [[0,1,2,3..],[0,1,2,3..] , ]\n",
    "                    sorted_ind = tf.cast(tf.tile(tf.expand_dims(tf.range(env.n_nodes),0),[batch_size,1]),tf.int64)\n",
    "                    tmp = tf.multiply(tf.cast(tf.greater(prob_idx_cum,rand_uni),tf.int64), sorted_ind)+\\\n",
    "                        10000*tf.cast(tf.greater_equal(rand_uni,prob_idx_cum),tf.int64)\n",
    "\n",
    "                    idx = tf.expand_dims(tf.argmin(input=tmp,axis=1),1)\n",
    "                    return tmp, idx\n",
    "\n",
    "                tmp, idx = my_multinomial()\n",
    "                # check validity of tmp -> True or False -- True mean take a new sample\n",
    "                tmp_check = tf.cast(tf.reduce_sum(input_tensor=tf.cast(tf.greater(tf.reduce_sum(input_tensor=tmp,axis=1),(10000*env.n_nodes)-1),\n",
    "                                                          tf.int32)),tf.bool)\n",
    "                tmp , idx = tf.cond(pred=tmp_check,true_fn=my_multinomial,false_fn=lambda:(tmp,idx))\n",
    "\n",
    "            elif decode_type == 'beam_search':\n",
    "                if i==0:\n",
    "                    # BatchBeamSeq: [batch_size*beam_width x 1]\n",
    "                    # [0,1,2,3,...,127,0,1,...],\n",
    "                    batchBeamSeq = tf.expand_dims(tf.tile(tf.cast(tf.range(batch_size), tf.int64),\n",
    "                                                         [beam_width]),1)\n",
    "                    beam_path  = []\n",
    "                    log_beam_probs = []\n",
    "                    # in the initial decoder step, we want to choose beam_width different branches\n",
    "                    # log_beam_prob: [batch_size, sourceL]\n",
    "                    log_beam_prob = tf.math.log(tf.split(prob,num_or_size_splits=beam_width, axis=0)[0])\n",
    "\n",
    "                elif i > 0:\n",
    "                    log_beam_prob = tf.math.log(prob) + log_beam_probs[-1]\n",
    "                    # log_beam_prob:[batch_size, beam_width*sourceL]\n",
    "                    log_beam_prob = tf.concat(tf.split(log_beam_prob, num_or_size_splits=beam_width, axis=0),1)\n",
    "\n",
    "                # topk_prob_val,topk_logprob_ind: [batch_size, beam_width]\n",
    "                topk_logprob_val, topk_logprob_ind = tf.nn.top_k(log_beam_prob, beam_width)\n",
    "\n",
    "                # topk_logprob_val , topk_logprob_ind: [batch_size*beam_width x 1]\n",
    "                topk_logprob_val = tf.transpose(a=tf.reshape(\n",
    "                    tf.transpose(a=topk_logprob_val), [1,-1]))\n",
    "\n",
    "                topk_logprob_ind = tf.transpose(a=tf.reshape(\n",
    "                    tf.transpose(a=topk_logprob_ind), [1,-1]))\n",
    "\n",
    "                #idx,beam_parent: [batch_size*beam_width x 1]\n",
    "                idx = tf.cast(topk_logprob_ind % env.n_nodes, tf.int64) # Which city in route.\n",
    "                beam_parent = tf.cast(topk_logprob_ind // env.n_nodes, tf.int64) # Which hypothesis it came from.\n",
    "\n",
    "                # batchedBeamIdx:[batch_size*beam_width]\n",
    "                batchedBeamIdx= batchBeamSeq + tf.cast(batch_size,tf.int64)*beam_parent\n",
    "                prob = tf.gather_nd(prob,batchedBeamIdx)\n",
    "\n",
    "                beam_path.append(beam_parent)\n",
    "                log_beam_probs.append(topk_logprob_val)\n",
    "\n",
    "            state = env.step(idx,beam_parent)\n",
    "            batched_idx = tf.concat([BatchSequence,idx],1)\n",
    "\n",
    "\n",
    "            decoder_input = tf.expand_dims(tf.gather_nd(\n",
    "                tf.tile(encoder_emb_inp,[beam_width,1,1]), batched_idx),1)\n",
    "\n",
    "            logprob = tf.math.log(tf.gather_nd(prob, batched_idx))\n",
    "            probs.append(prob)\n",
    "            idxs.append(idx)\n",
    "            logprobs.append(logprob)\n",
    "\n",
    "            action = tf.gather_nd(tf.tile(input_pnt, [beam_width,1,1]), batched_idx )\n",
    "            actions_tmp.append(action)\n",
    "\n",
    "        if decode_type=='beam_search':\n",
    "            # find paths of the beam search\n",
    "            tmplst = []\n",
    "            tmpind = [BatchSequence]\n",
    "            for k in reversed(range(len(actions_tmp))):\n",
    "\n",
    "                tmplst = [tf.gather_nd(actions_tmp[k],tmpind[-1])] + tmplst\n",
    "                tmpind += [tf.gather_nd(\n",
    "                    (batchBeamSeq + tf.cast(batch_size,tf.int64)*beam_path[k]),tmpind[-1])]\n",
    "            actions = tmplst\n",
    "\n",
    "        else:\n",
    "            actions = actions_tmp\n",
    "\n",
    "        if self.args['min_trucks']:\n",
    "            tile_input_pt = tf.tile(input_pnt[:,env.n_nodes-1,:],[beam_width,1])\n",
    "            R = self.reward_func(actions,args['decode_len'],self.args['n_nodes']-1,tile_input_pt)\n",
    "        else:\n",
    "            R = self.reward_func(actions)\n",
    "\n",
    "\n",
    "        ### critic\n",
    "        v = tf.constant(0)\n",
    "        if decode_type=='stochastic':\n",
    "            with tf.compat.v1.variable_scope(\"Critic\"):\n",
    "                with tf.compat.v1.variable_scope(\"Encoder\"):\n",
    "                    # init states\n",
    "                    initial_state = tf.zeros([args['rnn_layers'], 2, batch_size, args['hidden_dim']])\n",
    "                    l = tf.unstack(initial_state, axis=0)\n",
    "                    rnn_tuple_state = tuple([tf.compat.v1.nn.rnn_cell.LSTMStateTuple(l[idx][0],l[idx][1]) # index + corresponds to coord\n",
    "                              for idx in range(args['rnn_layers'])])\n",
    "\n",
    "                    hy = rnn_tuple_state[0][1]\n",
    "\n",
    "                with tf.compat.v1.variable_scope(\"Process\"):\n",
    "                    for i in range(args['n_process_blocks']):\n",
    "\n",
    "                        process = self.clAttentionCritic(args['hidden_dim'],_name=\"P\"+str(i))\n",
    "                        e,logit = process(hy, encoder_emb_inp, env)\n",
    "\n",
    "                        prob = tf.nn.softmax(logit)\n",
    "                        # hy : [batch_size x 1 x sourceL] * [batch_size  x sourceL x hidden_dim]  ->\n",
    "                        #[batch_size x h_dim ]\n",
    "                        hy = tf.squeeze(tf.matmul(tf.expand_dims(prob,1), e ) ,1)\n",
    "\n",
    "                with tf.compat.v1.variable_scope(\"Linear\"):\n",
    "                    v = tf.squeeze(tf.compat.v1.layers.dense(tf.compat.v1.layers.dense(hy,args['hidden_dim']\\\n",
    "                                                               ,tf.nn.relu,name='L1'),1,name='L2'),1)\n",
    "\n",
    "\n",
    "        return (R, v, logprobs, actions, idxs, env.input_pnt , probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe3916f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAgent( RLAgent ):\n",
    "    def build_train_step(self):\n",
    "        '''\n",
    "        This function returns a train_step op, in which by running it we proceed one training step.\n",
    "        '''\n",
    "        args = self.args\n",
    "\n",
    "        R, v, logprobs, actions, idxs , batch , probs= self.train_summary\n",
    "\n",
    "        v_nograd = tf.stop_gradient(v)\n",
    "        R = tf.stop_gradient(R)\n",
    "\n",
    "        # losses\n",
    "        actor_loss = tf.reduce_mean(input_tensor=tf.multiply((R-v_nograd),tf.add_n(logprobs)),axis=0)     # compute mean over the zero axis\n",
    "        critic_loss = tf.compat.v1.losses.mean_squared_error(R,v)\n",
    "\n",
    "        # optimizers\n",
    "        actor_optim = tf.compat.v1.train.AdamOptimizer(args['actor_net_lr'])\n",
    "        critic_optim = tf.compat.v1.train.AdamOptimizer(args['critic_net_lr'])\n",
    "\n",
    "        # compute gradients\n",
    "        actor_gra_and_var = actor_optim.compute_gradients(actor_loss,\\\n",
    "                                tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope='Actor'))\n",
    "        critic_gra_and_var = critic_optim.compute_gradients(critic_loss,\\\n",
    "                                tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope='Critic'))\n",
    "\n",
    "        # clip gradients\n",
    "        clip_actor_gra_and_var = [(tf.clip_by_norm(grad, args['max_grad_norm']), var) \\\n",
    "                                  for grad, var in actor_gra_and_var]\n",
    "\n",
    "        clip_critic_gra_and_var = [(tf.clip_by_norm(grad, args['max_grad_norm']), var) \\\n",
    "                                  for grad, var in critic_gra_and_var]\n",
    "\n",
    "        # apply gradients\n",
    "        actor_train_step = actor_optim.apply_gradients(clip_actor_gra_and_var)\n",
    "        critic_train_step = critic_optim.apply_gradients(clip_critic_gra_and_var)\n",
    "\n",
    "        train_step = [actor_train_step,\n",
    "                          critic_train_step ,\n",
    "                          actor_loss,\n",
    "                          critic_loss,\n",
    "                          actor_gra_and_var,\n",
    "                          critic_gra_and_var,\n",
    "                          R,\n",
    "                          v,\n",
    "                          logprobs,\n",
    "                          probs,\n",
    "                          actions,\n",
    "                          idxs]\n",
    "        return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c64ae4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAgent( RLAgent ):\n",
    "    def Initialize(self,sess):\n",
    "        self.sess = sess\n",
    "        self.sess.run(tf.compat.v1.global_variables_initializer())\n",
    "        self.load_model()\n",
    "\n",
    "    def load_model(self):\n",
    "        latest_ckpt = tf.train.latest_checkpoint(self.args['load_path'])\n",
    "        if latest_ckpt is not None:\n",
    "            print(\"have load model\")\n",
    "            self.saver.restore(self.sess, latest_ckpt)\n",
    "\n",
    "\n",
    "    def evaluate_single(self,eval_type='greedy'):\n",
    "        start_time = time.time()\n",
    "        avg_reward = []\n",
    "        all_output = []\n",
    "\n",
    "        if eval_type == 'greedy':\n",
    "            summary = self.val_summary_greedy\n",
    "        elif eval_type == 'beam_search':\n",
    "            summary = self.val_summary_beam\n",
    "        self.dataGen.reset()\n",
    "        for step in range(self.dataGen.n_problems):\n",
    "\n",
    "            data = self.dataGen.get_test_next()\n",
    "            input_concat = np.concatenate(data)\n",
    "            norm_by_feature = np.reshape(np.transpose(input_concat),(self.args['input_dim'],-1))\n",
    "            norm_by_feature = normalize(norm_by_feature, axis=1)\n",
    "            data_norm = np.reshape(np.transpose(norm_by_feature),(data.shape[0],data.shape[1],data.shape[2]))\n",
    "\n",
    "            if self.args['embedding_graph'] == 0:\n",
    "                dict_to_feed = {self.env.input_data:data,\n",
    "                                self.env.input_data_norm:data_norm,\n",
    "                                self.env.embeded_data: np.zeros(shape=(self.args['batch_size'],self.args['n_nodes'],self.args['embedding_dim'])),\n",
    "                                self.decodeStep.dropout:0.0}\n",
    "            elif self.args['embedding_graph'] == 1:\n",
    "                dict_to_feed = {self.env.input_data:data,\n",
    "                                self.env.input_data_norm:data_norm,\n",
    "                                self.env.embeded_data:self.embedder_model(data),\n",
    "                                self.decodeStep.dropout:0.0}\n",
    "            else:\n",
    "                dict_to_feed = {self.env.input_data:data,\n",
    "                                self.env.input_data_norm:data_norm,\n",
    "                                self.env.embeded_data: np.zeros(shape=(self.args['batch_size'],self.args['n_nodes'],self.args['embedding_dim'])),\n",
    "                                self.embedder_model.drop_out: 1.0,\n",
    "                                self.decodeStep.dropout:0.0}\n",
    "\n",
    "            R, v, logprobs, actions,idxs, batch, _= self.sess.run(summary,\n",
    "                                         feed_dict=dict_to_feed)\n",
    "            if eval_type=='greedy':\n",
    "                avg_reward.append(R)\n",
    "                R_ind0 = 0\n",
    "            elif eval_type=='beam_search':\n",
    "                # R : [batch_size x beam_width]\n",
    "                R = np.concatenate(np.split(np.expand_dims(R,1) ,self.args['beam_width'], axis=0),1 )\n",
    "                R_val = np.amin(R,1, keepdims = False)\n",
    "                R_ind0 = np.argmin(R,1)[0]\n",
    "                avg_reward.append(R_val)\n",
    "\n",
    "            # print decode in file data\n",
    "            example_output = [list(batch[0, self.env.n_nodes-1, :])] # we begin by the depot\n",
    "            for idx, action in enumerate(actions):\n",
    "                example_output.append(list(action[R_ind0*np.shape(batch)[0]]))\n",
    "            all_output.append(example_output)\n",
    "\n",
    "\n",
    "            # sample decode\n",
    "            if step % int(self.args['log_interval']) == 0:\n",
    "                example_output = []\n",
    "                example_input = []\n",
    "                for i in range(self.env.n_nodes):\n",
    "                    example_input.append(list(batch[0, i, :]))\n",
    "                for idx, action in enumerate(actions):\n",
    "                    example_output.append(list(action[R_ind0*np.shape(batch)[0]]))\n",
    "                self.prt.print_out('\\n\\nVal-Step of {}: {}'.format(eval_type,step))\n",
    "                self.prt.print_out('\\nExample test input: {}'.format(example_input))\n",
    "                self.prt.print_out('\\nExample test output: {}'.format(example_output))\n",
    "                self.prt.print_out('\\nExample test reward: {} - best: {}'.format(R[0],R_ind0))\n",
    "\n",
    "        end_time = time.time() - start_time\n",
    "\n",
    "        # Finished going through the iterator dataset.\n",
    "        self.prt.print_out('\\nValidation overall avg_reward: {}'.format(np.mean(avg_reward)) )\n",
    "        self.prt.print_out('Validation overall reward std: {}'.format(np.sqrt(np.var(avg_reward))) )\n",
    "\n",
    "        self.prt.print_out(\"Finished evaluation with %d steps in %s.\" % (step\\\n",
    "                           ,time.strftime(\"%H:%M:%S\", time.gmtime(end_time))))\n",
    "\n",
    "        # Ouputting the results\n",
    "        self._output_results(all_output,eval_type)\n",
    "\n",
    "\n",
    "    def _output_results(self,all_ouput,eval_type):\n",
    "        \"\"\"\n",
    "        Output the deconding results obtained after a single inference\n",
    "        :param all_ouput: list of routes, in order\n",
    "        :param eval_type: the type (greedy or beam_search)\n",
    "        \"\"\"\n",
    "        # create directory\n",
    "        dir_name = os.path.join(self.args['log_dir'],'results')\n",
    "        if not os.path.exists(dir_name):\n",
    "            os.mkdir(dir_name)\n",
    "\n",
    "        task = self.args['task_name']\n",
    "        # build task name and datafiles\n",
    "        if self.args['ups']:\n",
    "            task_name = '{}-ups-size-{}-len-{}-results-{}.txt'.format(task,self.args['test_size'], self.env.n_nodes,eval_type)\n",
    "        else:\n",
    "            task_name = '{}-size-{}-len-{}-results-{}.txt'.format(task,self.args['test_size'], self.env.n_nodes,eval_type)\n",
    "        fname = os.path.join(self.args['log_dir'],'results', task_name)\n",
    "\n",
    "        input_file =open(fname, 'w')\n",
    "        for output in all_ouput:\n",
    "            depot_x = output[0][0]\n",
    "            depot_y = output[0][1]\n",
    "\n",
    "            nb_stop = 0\n",
    "            for node in output:\n",
    "                if task == 'vrp':\n",
    "                    input_file.write(str(node[0]) + \" \" + str(node[1]) + \" \")\n",
    "                elif task =='vrptw':\n",
    "                    input_file.write(str(node[0]) + \" \" + str(node[1]) + \" \" + str(node[2]) + \" \" + str(node[3]) + \" \")\n",
    "                else:\n",
    "                    assert False\n",
    "                # check if depot or stop\n",
    "                if abs(depot_x - node[0]) >= 0.001 or abs(depot_y - node[1]) >= 0.001:\n",
    "                    nb_stop +=1\n",
    "\n",
    "                if nb_stop == self.env.n_nodes -1:\n",
    "                    # we have found all the stops so write depot again and break\n",
    "                    if task == 'vrp':\n",
    "                        input_file.write(str(depot_x) + \" \" + str(depot_y))\n",
    "                    elif task =='vrptw':\n",
    "                        depot_b_tw = output[0][2]\n",
    "                        depot_e_tw = output[0][3]\n",
    "                        input_file.write(str(depot_x) + \" \" + str(depot_y) + \" \" + str(depot_b_tw) + \" \" + str(depot_e_tw))\n",
    "                    break\n",
    "            input_file.write(\"\\n\")\n",
    "        input_file.close()\n",
    "\n",
    "        # copy the input file\n",
    "        if self.args['ups']:\n",
    "            copy_name = '{}-ups-size-{}-len-{}-test.txt'.format(task,self.args['test_size'], self.env.n_nodes)\n",
    "        else:\n",
    "            copy_name = '{}-size-{}-len-{}-test.txt'.format(task,self.args['test_size'], self.env.n_nodes)\n",
    "        old_loc = os.path.join(self.args['data_dir'], copy_name)\n",
    "        new_loc = os.path.join(self.args['log_dir'],'results', copy_name)\n",
    "        copyfile(old_loc,new_loc)\n",
    "\n",
    "\n",
    "\n",
    "    def evaluate_batch(self,eval_type='greedy'):\n",
    "        self.env.reset()\n",
    "        if eval_type == 'greedy':\n",
    "            summary = self.val_summary_greedy\n",
    "            beam_width = 1\n",
    "        elif eval_type == 'beam_search':\n",
    "            summary = self.val_summary_beam\n",
    "            beam_width = self.args['beam_width']\n",
    "\n",
    "\n",
    "        data = self.dataGen.get_test_all()\n",
    "        input_concat = np.concatenate(data)\n",
    "        norm_by_feature = np.reshape(np.transpose(input_concat),(self.args['input_dim'],-1))\n",
    "        norm_by_feature = normalize(norm_by_feature, axis=1)\n",
    "        data_norm = np.reshape(np.transpose(norm_by_feature),(data.shape[0],data.shape[1],data.shape[2]))\n",
    "\n",
    "        if self.args['embedding_graph'] == 0:\n",
    "            dict_to_feed = {self.env.input_data:data,\n",
    "                            self.env.input_data_norm:data_norm,\n",
    "                            self.env.embeded_data: np.zeros(shape=(self.args['batch_size'],self.args['n_nodes'],self.args['embedding_dim'])),\n",
    "                            self.decodeStep.dropout:0.0}\n",
    "        elif self.args['embedding_graph'] == 1:\n",
    "            dict_to_feed = {self.env.input_data:data,\n",
    "                            self.env.input_data_norm:data_norm,\n",
    "                            self.env.embeded_data:self.embedder_model(data),\n",
    "                            self.decodeStep.dropout:0.0}\n",
    "        else:\n",
    "            dict_to_feed = {self.env.input_data:data,\n",
    "                            self.env.input_data_norm:data_norm,\n",
    "                            self.env.embeded_data: np.zeros(shape=(self.args['batch_size'],self.args['n_nodes'],self.args['embedding_dim'])),\n",
    "                            self.embedder_model.drop_out: 1.0,\n",
    "                            self.decodeStep.dropout:0.0}\n",
    "\n",
    "        start_time = time.time()\n",
    "        R, v, logprobs, actions,idxs, batch, _= self.sess.run(summary,\n",
    "                                     feed_dict=dict_to_feed)\n",
    "\n",
    "        R = np.concatenate(np.split(np.expand_dims(R,1) ,beam_width, axis=0),1 )\n",
    "        R = np.amin(R,1, keepdims = False)\n",
    "\n",
    "        end_time = time.time() - start_time\n",
    "        self.prt.print_out('Average of {} in batch-mode: {} -- std {} -- time {} s'.format(eval_type,\\\n",
    "            np.mean(R),np.sqrt(np.var(R)),end_time))\n",
    "        self.out_avg_resul.write(eval_type + '_' + str(np.mean(R)) + '\\n')\n",
    "\n",
    "    def inference(self, infer_type='batch'):\n",
    "        if infer_type == 'batch':\n",
    "            self.evaluate_batch('greedy')\n",
    "            self.evaluate_batch('beam_search')\n",
    "        elif infer_type == 'single':\n",
    "            self.evaluate_single('greedy')\n",
    "            self.evaluate_single('beam_search')\n",
    "        self.prt.print_out(\"##################################################################\")\n",
    "\n",
    "\n",
    "    def run_train_step(self):\n",
    "        data = self.dataGen.get_train_next()\n",
    "        input_concat = np.concatenate(data)\n",
    "        before_norm_by_feature = np.reshape(np.transpose(input_concat),(self.args['input_dim'],-1))\n",
    "        norm_by_feature = normalize(before_norm_by_feature, axis=1)\n",
    "        data_norm = np.reshape(np.transpose(norm_by_feature),(data.shape[0],data.shape[1],data.shape[2]))\n",
    "\n",
    "        if self.args['embedding_graph'] == 0:\n",
    "            dict_to_feed = {self.env.input_data:data,\n",
    "                            self.env.input_data_norm:data_norm,\n",
    "                            self.env.embeded_data: np.zeros(shape=(self.args['batch_size'],self.args['n_nodes'],self.args['embedding_dim'])),\n",
    "                            self.decodeStep.dropout:self.args['dropout']}\n",
    "        elif self.args['embedding_graph'] == 1:\n",
    "            dict_to_feed = {self.env.input_data:data,\n",
    "                            self.env.input_data_norm:data_norm,\n",
    "                            self.env.embeded_data:self.embedder_model(data),\n",
    "                            self.decodeStep.dropout:self.args['dropout']}\n",
    "        else:\n",
    "            dict_to_feed = {self.env.input_data:data,\n",
    "                            self.env.input_data_norm:data_norm,\n",
    "                            self.env.embeded_data: np.zeros(shape=(self.args['batch_size'],self.args['n_nodes'],self.args['embedding_dim'])),\n",
    "                            self.embedder_model.drop_out: 0.8,\n",
    "                            self.decodeStep.dropout:self.args['dropout']}\n",
    "\n",
    "        train_results = self.sess.run(self.train_step,\n",
    "                                 feed_dict=dict_to_feed)\n",
    "        return train_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "562b73db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(object):\n",
    "    '''\n",
    "    This class is the base class for embedding the input graph.\n",
    "    '''\n",
    "    def __init__(self,emb_type, embedding_dim):\n",
    "        self.emb_type = emb_type\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.total_time = 0\n",
    "\n",
    "    def __call__(self,input_pnt):\n",
    "        # returns the embeded tensor. Should be implemented in child classes\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52a987c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearEmbedding(Embedding):\n",
    "    '''\n",
    "    This class implements linear embedding. It is only a mapping \n",
    "    to a higher dimensional space.\n",
    "    '''\n",
    "    def __init__(self,embedding_dim,_scope=''):\n",
    "        '''\n",
    "        Input: \n",
    "            embedding_dim: embedding dimension\n",
    "        '''\n",
    "\n",
    "        super(LinearEmbedding,self).__init__('linear',embedding_dim)\n",
    "        self.project_emb = tf.compat.v1.layers.Conv1D(embedding_dim,1,\n",
    "            _scope=_scope+'Embedding/conv1d')\n",
    "\n",
    "    def __call__(self,input_pnt):\n",
    "        # emb_inp_pnt: [batch_size, max_time, embedding_dim]\n",
    "        time_init = time.time()\n",
    "        emb_inp_pnt = self.project_emb(input_pnt)\n",
    "        # emb_inp_pnt = tf.Print(emb_inp_pnt,[emb_inp_pnt])\n",
    "        self.total_time += time.time() - time_init\n",
    "        return emb_inp_pnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a33567-3118-45c5-bcf0-902e6e14d736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "233e85d1-3507-4f41-8bb9-7e229059a305",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphEmbedding(Embedding):\n",
    "    \"\"\"\n",
    "    This class implement a graph embedding. The specificity is that it has already been optimized on\n",
    "    another task. Implementation of transfer learning.\n",
    "    \"\"\"\n",
    "    def __init__(self,args,data_test):\n",
    "        assert args['embedding_dim'] == 30, args['embedding_dim']\n",
    "        super(GraphEmbedding, self).__init__('graph',embedding_dim=args['embedding_dim'])\n",
    "\n",
    "        self.n_nodes = args['n_nodes']\n",
    "        self.embedding_dim =  args['embedding_dim']\n",
    "        model_path = 'shared/graph_embedding/model_storage/' + args['task'] + '_model.pickle'\n",
    "        result_dir = args['log_dir'] + '/embedding/'\n",
    "        os.makedirs(result_dir)\n",
    "        self.graph_model = self.restore(model_path, result_dir)\n",
    "        self.graph_model.params['max_nodes_in_batch'] = args['test_size'] * self.n_nodes + 10 # We can process larger batches if we don't do training\n",
    "        self.embedded_data = self(data_test)\n",
    "\n",
    "\n",
    "    def __call__(self, input_data):\n",
    "        \"\"\"\n",
    "        :param input_data: the input data as given by the env i.e. [batch_size x max_time x dim_task]\n",
    "        :return: an embedding corresponding to the final node represenatation obtained via transfer learning.\n",
    "        \"\"\"\n",
    "        time_init = time.time()\n",
    "        embedded_data = self.graph_model.test(input_data)\n",
    "        embedded_data = np.reshape(embedded_data,(-1,self.n_nodes,self.embedding_dim))\n",
    "        self.total_time += time.time() - time_init\n",
    "\n",
    "        return embedded_data\n",
    "\n",
    "    @staticmethod\n",
    "    def restore(saved_model_path: str, result_dir: str, run_id: str = None):\n",
    "        print(\"Loading model from file %s.\" % saved_model_path)\n",
    "        with open(saved_model_path, 'rb') as in_file:\n",
    "            data_to_load = pickle.load(in_file)\n",
    "\n",
    "        # model_cls, _ = name_to_model_class(data_to_load['model_class']({}))   # before...\n",
    "        model_cls = GNN_FiLM_Model\n",
    "        task_cls, additional_task_params = Nb_Vehicles_Task, {\"data_kind\":'transfer_learning'}\n",
    "\n",
    "        if run_id is None:\n",
    "            run_id = \"_\".join([task_cls.name(), model_cls.name(data_to_load['model_params']), time.strftime(\"%Y-%m-%d-%H-%M-%S\"), str(os.getpid())])\n",
    "\n",
    "        task = task_cls(data_to_load['task_params'])\n",
    "        task.restore_from_metadata(data_to_load['task_metadata'])\n",
    "\n",
    "        model = model_cls(data_to_load['model_params'], task, run_id, result_dir)\n",
    "        model.load_weights(data_to_load['weights'])\n",
    "\n",
    "        model.log_line(\"Loaded model from snapshot %s.\" % saved_model_path)\n",
    "\n",
    "        return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88ebdd54-072b-4eeb-a26e-5e1c734652e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import shared.misc_utils as utils\n",
    "import os\n",
    "from task_specific_params import task_lst\n",
    "\n",
    "def str2bool(v):\n",
    "    return v.lower() in ('true', '1')\n",
    "\n",
    "def initialize_task_settings(args,task):\n",
    "\n",
    "    try:\n",
    "        task_params = task_lst[task]\n",
    "    except:\n",
    "        raise Exception('Task is not implemented.')\n",
    "\n",
    "    for name, value in task_params._asdict().items():\n",
    "    \targs[name] = value\n",
    "\n",
    "\n",
    "    # args['task_name'] = task_params.task_name\n",
    "    # args['input_dim'] = task_params.input_dim\n",
    "    # args['n_nodes'] = task_params.n_nodes\n",
    "    # if args['decode_len'] == None:\n",
    "    #     args['decode_len'] = task_params.decode_len\n",
    "\n",
    "    return args\n",
    "\n",
    "def ParseParams():\n",
    "    parser = argparse.ArgumentParser(description=\"Neural Combinatorial Optimization with RL\")\n",
    "\n",
    "    # Data\n",
    "    #parser.add_argument('--task', default='vrp10', help=\"Select the task to solve; i.e. vrp10\")\n",
    "    parser.add_argument('--task', default='vrp10', help=\"Select the task to solve; i.e. vrptw50\")\n",
    "    parser.add_argument('--ups',default=False,help='decide if we are going to train on the distribution infer from ups')\n",
    "    parser.add_argument('--batch_size', default=6,type=int, help='Batch size in training') #Yifei: for study purpose, reduce the batchsize to 1, to simplify it \n",
    "    #parser.add_argument('--batch_size', default=128,type=int, help='Batch size in training')\n",
    "    parser.add_argument('--n_train', default=10,type=int, help='Number of training steps') #Yifeiï¼šfor study purpose,  reduce the training step to 10\n",
    "    #parser.add_argument('--n_train', default=260000,type=int, help='Number of training steps')\n",
    "    parser.add_argument('--test_size', default=1000,type=int, help='Number of problems in test set')\n",
    "\n",
    "    # Network\n",
    "    parser.add_argument('--agent_type', default='attention', help=\"attention|pointer\")\n",
    "    parser.add_argument('--forget_bias', default=1.0,type=float, help=\"Forget bias for BasicLSTMCell.\")\n",
    "    parser.add_argument('--embedding_dim', default=30,type=int, help='Dimension of input embedding')\n",
    "    parser.add_argument('--embedding_graph', default=2, type=int, help='Embedding either relying on GNN or CNN')\n",
    "    parser.add_argument('--hidden_dim', default=128,type=int, help='Dimension of hidden layers in Enc/Dec')\n",
    "    parser.add_argument('--n_process_blocks', default=3,type=int,\n",
    "                        help='Number of process block iters to run in the Critic network')\n",
    "    parser.add_argument('--rnn_layers', default=1, type=int, help='Number of LSTM layers in the encoder and decoder')\n",
    "    parser.add_argument('--decode_len', default=None,type=int,\n",
    "                        help='Number of time steps the decoder runs before stopping')\n",
    "    parser.add_argument('--n_glimpses', default=0, type=int, help='Number of glimpses to use in the attention')\n",
    "    parser.add_argument('--tanh_exploration', default=10.,  type=float,\n",
    "             help='Hyperparam controlling exploration in the net by scaling the tanh in the softmax')\n",
    "    parser.add_argument('--use_tanh', type=str2bool, default=False, help='')\n",
    "    parser.add_argument('--mask_glimpses', type=str2bool, default=True, help='')\n",
    "    parser.add_argument('--mask_pointer', type=str2bool, default=True, help='')\n",
    "    parser.add_argument('--dropout', default=0.1, type=float, help='The dropout prob')\n",
    "\n",
    "    # Training\n",
    "    parser.add_argument('--is_train', default=True,type=str2bool, help=\"whether to do the training or not\")\n",
    "    parser.add_argument('--actor_net_lr', default=1e-4,type=float, help=\"Set the learning rate for the actor network\")\n",
    "    parser.add_argument('--critic_net_lr', default=1e-4,type=float, help=\"Set the learning rate for the critic network\")\n",
    "    parser.add_argument('--random_seed', default=24601,type=int, help='')\n",
    "    parser.add_argument('--max_grad_norm', default=2.0, type=float, help='Gradient clipping')\n",
    "    parser.add_argument('--entropy_coeff', default=0.0, type=float, help='coefficient for entropy regularization')\n",
    "    parser.add_argument('--min_trucks', default=False, type=str2bool, help='True to minimize trucks in reward func')\n",
    "    # parser.add_argument('--loss_type', type=int, default=1, help='1,2,3')\n",
    "\n",
    "    # inference\n",
    "    parser.add_argument('--infer_type', default='batch',\n",
    "        help='single|batch: do inference for the problems one-by-one, or run it all at once')\n",
    "    parser.add_argument('--beam_width', default=5, type=int, help='')\n",
    "\n",
    "    # Misc\n",
    "    parser.add_argument('--stdout_print', default=True, type=str2bool, help='print control')\n",
    "    parser.add_argument(\"--gpu\", default='3', type=str,help=\"gpu number.\")\n",
    "    parser.add_argument('--log_interval', default=200,type=int, help='Log info every log_step steps')\n",
    "    parser.add_argument('--test_interval', default=200,type=int, help='test every test_interval steps')\n",
    "    parser.add_argument('--save_interval', default=1000,type=int, help='save every save_interval steps')\n",
    "    parser.add_argument('--log_dir', type=str, default='logs')\n",
    "    parser.add_argument('--data_dir', type=str, default='./data')\n",
    "    #parser.add_argument('--model_dir', type=str, default='E:\\GitHub\\TF4AI_practice\\vrp\\logs\\vrp10-2021-08-15_23-07-57\\model')\n",
    "    parser.add_argument('--model_dir', type=str, default='')  #Yifei study purpose\n",
    "    parser.add_argument('--load_path', type=str, default='', help='Path to load trained variables')\n",
    "    parser.add_argument('--disable_tqdm', default=True, type=str2bool)\n",
    "\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    args = vars(args)\n",
    "\n",
    "    args['log_dir'] = \"{}/{}-{}\".format(args['log_dir'],args['task'], utils.get_time())\n",
    "    if args['model_dir'] =='':\n",
    "        args['model_dir'] = os.path.join(args['log_dir'],'model')\n",
    "\n",
    "    # file to write the stdout\n",
    "    try:\n",
    "        os.makedirs(args['log_dir'])\n",
    "        os.makedirs(args['model_dir'])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # create a print handler\n",
    "    out_file = open(os.path.join(args['log_dir'], 'results.txt'),'w+')\n",
    "    prt = utils.printOut(out_file,args['stdout_print'])\n",
    "\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=  args['gpu']\n",
    "\n",
    "    args = initialize_task_settings(args,args['task'])\n",
    "\n",
    "    # print the run args\n",
    "    for key, value in sorted(args.items()):\n",
    "        prt.print_out(\"{}: {}\".format(key,value))\n",
    "\n",
    "    return args, prt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44e311fe-e265-4053-bfa9-067060baad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    args, prt = ParseParams()\n",
    "    data_Gen = DataGenerator(args)\n",
    "    # print(data_Gen.test_data)\n",
    "    print(data_Gen.test_data.shape)\n",
    "\n",
    "    graph_embedding = GraphEmbedding(args,data_Gen.test_data)\n",
    "    data = data_Gen.get_train_next()\n",
    "    graph_embedding(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b018f486-b78d-4b7e-8962-e8d88880d734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from VRP.vrp_utils import create_VRP_dataset\n",
    "class DataGenerator(object):\n",
    "    def __init__(self,\n",
    "                 args):\n",
    "\n",
    "        '''\n",
    "        This class generates VRP problems for training and test\n",
    "        Inputs:\n",
    "            args: the parameter dictionary. It should include:\n",
    "                args['random_seed']: random seed\n",
    "                args['test_size']: number of problems to test\n",
    "                args['n_nodes']: number of nodes\n",
    "                args['n_cust']: number of customers\n",
    "                args['batch_size']: batchsize for training\n",
    "        '''\n",
    "        self.args = args\n",
    "        assert not self.args['ups']\n",
    "        self.rnd = np.random.RandomState(seed= args['random_seed'])\n",
    "        print('Created train iterator.')\n",
    "\n",
    "        # create test data\n",
    "        self.n_problems = args['test_size']\n",
    "        self.test_data = create_VRP_dataset(self.n_problems,args['n_cust'],args['data_dir'],\n",
    "            seed = args['random_seed']+1,data_type='test')\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.count = 0\n",
    "\n",
    "    def get_train_next(self):\n",
    "        '''\n",
    "        Get next batch of problems for training\n",
    "        Retuens:\n",
    "            input_data: data with shape [batch_size x max_time x 3]\n",
    "        '''\n",
    "\n",
    "        input_pnt = self.rnd.uniform(0,1,\n",
    "            size=(self.args['batch_size'],self.args['n_nodes'],2))\n",
    "\n",
    "        demand = self.rnd.randint(1,10,[self.args['batch_size'],self.args['n_nodes']])\n",
    "        demand[:,-1]=0 # demand of depot\n",
    "\n",
    "        input_data = np.concatenate([input_pnt,np.expand_dims(demand,2)],2)\n",
    "\n",
    "        return input_data\n",
    "\n",
    "\n",
    "    def get_test_next(self):\n",
    "        '''\n",
    "        Get next batch of problems for testing\n",
    "        '''\n",
    "        if self.count<self.args['test_size']:\n",
    "            input_pnt = self.test_data[self.count:self.count+1]\n",
    "            self.count +=1\n",
    "        else:\n",
    "            warnings.warn(\"The test iterator reset.\")\n",
    "            self.count = 0\n",
    "            input_pnt = self.test_data[self.count:self.count+1]\n",
    "            self.count +=1\n",
    "\n",
    "        return input_pnt\n",
    "\n",
    "    def get_test_all(self):\n",
    "        '''\n",
    "        Get all test problems\n",
    "        '''\n",
    "        return self.test_data\n",
    "\n",
    "\n",
    "class State(collections.namedtuple(\"State\",\n",
    "                                        (\"load\",\n",
    "                                         \"demand\",\n",
    "                                         'd_sat',\n",
    "                                         \"mask\"))):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4316c8b-6466-4dd0-90b3-6d95501f8d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env(object):\n",
    "    def __init__(self,\n",
    "                 args):\n",
    "        '''\n",
    "        This is the environment for VRP.\n",
    "        Inputs:\n",
    "            args: the parameter dictionary. It should include:\n",
    "                args['n_nodes']: number of nodes in VRP\n",
    "                args['n_custs']: number of customers in VRP\n",
    "                args['input_dim']: dimension of the problem which is 3\n",
    "        '''\n",
    "        self.capacity = args['capacity']\n",
    "        self.n_nodes = args['n_nodes']\n",
    "        self.n_cust = args['n_cust']\n",
    "        self.input_dim = args['input_dim']\n",
    "        self.input_data = tf.compat.v1.placeholder(tf.float32,\\\n",
    "            shape=[None,self.n_nodes,self.input_dim])       # The dimension of the first (None) can be of any size\n",
    "\n",
    "        self.embeded_data = tf.compat.v1.placeholder(tf.float32,shape=[None,self.n_nodes,args['embedding_dim']])\n",
    "        self.input_data_norm = tf.compat.v1.placeholder(tf.float32,\\\n",
    "            shape=[None,self.n_nodes,self.input_dim])       # The dimension of the first (None) can be of any size\n",
    "\n",
    "        self.input_pnt = self.input_data[:,:,:(self.input_dim -1)]      # all but demand\n",
    "        self.demand = self.input_data[:,:,-1]\n",
    "        self.batch_size = tf.shape(input=self.input_pnt)[0]\n",
    "\n",
    "    def reset(self,beam_width=1):\n",
    "        '''\n",
    "        Resets the environment. This environment might be used with different decoders.\n",
    "        In case of using with beam-search decoder, we need to have to increase\n",
    "        the rows of the mask by a factor of beam_width.\n",
    "        '''\n",
    "\n",
    "        # dimensions\n",
    "        self.beam_width = beam_width\n",
    "        self.batch_beam = self.batch_size * beam_width\n",
    "\n",
    "        self.input_pnt = self.input_data[:,:,:2]        # corresponds to all x,y\n",
    "        self.demand = self.input_data[:,:,-1]           # corresponds to all the demand, sixe[batch,nb_nodes]\n",
    "\n",
    "        # modify the self.input_pnt and self.demand for beam search decoder\n",
    "#         self.input_pnt = tf.tile(self.input_pnt, [self.beam_width,1,1])\n",
    "\n",
    "        # demand: [batch_size * beam_width, max_time]\n",
    "        # demand[i] = demand[i+batchsize]\n",
    "        self.demand = tf.tile(self.demand, [self.beam_width,1])\n",
    "\n",
    "        # load: [batch_size * beam_width]\n",
    "        self.load = tf.ones([self.batch_beam])*self.capacity\n",
    "\n",
    "        # create mask\n",
    "        self.mask = tf.zeros([self.batch_size*beam_width,self.n_nodes],\n",
    "                dtype=tf.float32)\n",
    "\n",
    "        # update mask -- mask if customer demand is 0 and depot\n",
    "        self.mask = tf.concat([tf.cast(tf.equal(self.demand,0), tf.float32)[:,:-1],\n",
    "            tf.ones([self.batch_beam,1])],1)\n",
    "\n",
    "        state = State(load=self.load,\n",
    "                    demand = self.demand,\n",
    "                    d_sat = tf.zeros([self.batch_beam,self.n_nodes]),\n",
    "                    mask = self.mask )\n",
    "\n",
    "        return state\n",
    "\n",
    "    def step(self,\n",
    "             idx,\n",
    "             beam_parent=None):\n",
    "        '''\n",
    "        runs one step of the environment and updates demands, loads and masks\n",
    "        '''\n",
    "\n",
    "        # if the environment is used in beam search decoder\n",
    "        if beam_parent is not None:\n",
    "            # BatchBeamSeq: [batch_size*beam_width x 1]\n",
    "            # [0,1,2,3,...,127,0,1,...],\n",
    "            batchBeamSeq = tf.expand_dims(tf.tile(tf.cast(tf.range(self.batch_size), tf.int64),\n",
    "                                                 [self.beam_width]),1)\n",
    "            # batchedBeamIdx:[batch_size*beam_width]\n",
    "            batchedBeamIdx= batchBeamSeq + tf.cast(self.batch_size,tf.int64)*beam_parent\n",
    "            # demand:[batch_size*beam_width x sourceL]\n",
    "            self.demand= tf.gather_nd(self.demand,batchedBeamIdx)\n",
    "            #load:[batch_size*beam_width]\n",
    "            self.load = tf.gather_nd(self.load,batchedBeamIdx)\n",
    "            #MASK:[batch_size*beam_width x sourceL]\n",
    "            self.mask = tf.gather_nd(self.mask,batchedBeamIdx)\n",
    "\n",
    "\n",
    "        BatchSequence = tf.expand_dims(tf.cast(tf.range(self.batch_beam), tf.int64), 1)\n",
    "        batched_idx = tf.concat([BatchSequence,idx],1)\n",
    "\n",
    "        # how much the demand is satisfied\n",
    "        d_sat = tf.minimum(tf.gather_nd(self.demand,batched_idx), self.load)\n",
    "\n",
    "        # update the demand\n",
    "        d_scatter = tf.scatter_nd(batched_idx, d_sat, tf.cast(tf.shape(input=self.demand),tf.int64))      # sparse tensor containing d_sat for the interesting idx\n",
    "        self.demand = tf.subtract(self.demand, d_scatter)\n",
    "\n",
    "        # update load\n",
    "        self.load -= d_sat\n",
    "\n",
    "        # refill the truck -- idx: [10,9,10] -> load_flag: [1 0 1]\n",
    "        load_flag = tf.squeeze(tf.cast(tf.equal(idx,self.n_cust),tf.float32),1)\n",
    "        self.load = tf.multiply(self.load,1-load_flag) + load_flag *self.capacity\n",
    "\n",
    "        # mask for customers with zero demand\n",
    "        self.mask = tf.concat([tf.cast(tf.equal(self.demand,0), tf.float32)[:,:-1],\n",
    "                                          tf.zeros([self.batch_beam,1])],1)\n",
    "\n",
    "        # mask if load= 0\n",
    "        # mask if in depot and there is still a demand\n",
    "\n",
    "        self.mask += tf.concat( [tf.tile(tf.expand_dims(tf.cast(tf.equal(self.load,0),\n",
    "            tf.float32),1), [1,self.n_cust]),\n",
    "            tf.expand_dims(tf.multiply(tf.cast(tf.greater(tf.reduce_sum(input_tensor=self.demand,axis=1),0),tf.float32),\n",
    "                             tf.squeeze( tf.cast(tf.equal(idx,self.n_cust),tf.float32))),1)],1)\n",
    "\n",
    "        state = State(load=self.load,\n",
    "                    demand = self.demand,\n",
    "                    d_sat = d_sat,\n",
    "                    mask = self.mask )\n",
    "\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c720f6e4-6ebf-481a-b7e8-e9e84e1b6e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_func(sample_solution, decode_len=0.0, n_nodes=0.0, depot=None):\n",
    "    \"\"\"The reward for the VRP task is defined as the\n",
    "    negative value of the route length\n",
    "\n",
    "    Args:\n",
    "        sample_solution : a list tensor of size decode_len of shape [batch_size x input_dim]\n",
    "        demands satisfied: a list tensor of size decode_len of shape [batch_size]\n",
    "\n",
    "    Returns:\n",
    "        rewards: tensor of size [batch_size]\n",
    "\n",
    "    Example:\n",
    "        sample_solution = [[[1,1],[2,2]],[[3,3],[4,4]],[[5,5],[6,6]]]\n",
    "        sourceL = 3\n",
    "        batch_size = 2\n",
    "        input_dim = 2\n",
    "        sample_solution_tilted[ [[5,5]\n",
    "                                                    #  [6,6]]\n",
    "                                                    # [[1,1]\n",
    "                                                    #  [2,2]]\n",
    "                                                    # [[3,3]\n",
    "                                                    #  [4,4]] ]\n",
    "    \"\"\"\n",
    "    # make init_solution of shape [sourceL x batch_size x input_dim]\n",
    "    if depot != None:\n",
    "        counter = tf.zeros_like(depot)[:,0]\n",
    "        depot_visits = tf.cast(tf.equal(sample_solution[0], depot), tf.float32)[:,0]\n",
    "        for i in range(1,len(sample_solution)):\n",
    "            interm_depot = tf.cast(tf.equal(sample_solution[i], depot), tf.float32)[:,0]\n",
    "            counter = tf.add(tf.multiply(counter,interm_depot), interm_depot)\n",
    "            depot_visits = tf.add(depot_visits, tf.multiply(interm_depot, tf.cast(tf.less(counter,1.5), tf.float32)))\n",
    "            # depot_visits = tf.add(depot_visits,tf.cast(tf.equal(sample_solution[i], depot), tf.float32)[:,0])\n",
    "\n",
    "        max_length = tf.stack([depot for d in range(decode_len)],0)\n",
    "        max_lens_decoded = tf.reduce_sum(input_tensor=tf.pow(tf.reduce_sum(input_tensor=tf.pow(\\\n",
    "            (max_length - sample_solution) ,2), axis=2) , .5), axis=0)\n",
    "\n",
    "    # make sample_solution of shape [sourceL x batch_size x input_dim]\n",
    "    sample_solution = tf.stack(sample_solution,0)\n",
    "\n",
    "    sample_solution_tilted = tf.concat((tf.expand_dims(sample_solution[-1],0),\n",
    "         sample_solution[:-1]),0)\n",
    "    # get the reward based on the route lengths\n",
    "    # it's calculating the sum  of distance between every pair of continuouos stops, \n",
    "    # so it's actually the euclidean distance of each solution\n",
    "    route_lens_decoded = tf.reduce_sum(input_tensor=tf.pow(tf.reduce_sum(input_tensor=tf.pow(\\\n",
    "        (sample_solution_tilted - sample_solution) ,2), axis=2) , .5), axis=0)\n",
    "\n",
    "    if depot != None:\n",
    "        reward = tf.add(tf.scalar_mul(70.0,tf.scalar_mul(1.0/n_nodes,depot_visits)),tf.scalar_mul(30.0,tf.divide(route_lens_decoded,max_lens_decoded)))\n",
    "        return reward\n",
    "    else:\n",
    "        return route_lens_decoded\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8884b6c3-850d-4d6c-9261-89e8b70822a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionVRPActor(object):\n",
    "    \"\"\"A generic attention module for the attention in vrp model\"\"\"\n",
    "    def __init__(self, dim, use_tanh=False, C=10,_name='Attention',_scope=''):\n",
    "        self.use_tanh = use_tanh\n",
    "        self._scope = _scope\n",
    "\n",
    "        with tf.compat.v1.variable_scope(_scope+_name):\n",
    "            # self.v: is a variable with shape [1 x dim]\n",
    "            self.v = tf.compat.v1.get_variable('v',[1,dim],\n",
    "                       initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "            self.v = tf.expand_dims(self.v,2)\n",
    "\n",
    "        self.emb_d = tf.compat.v1.layers.Conv1D(dim,1,_scope=_scope+_name+'/emb_d' ) #conv1d of kernel size = dim, stride = 1\n",
    "                                                                                     # here should be filters = dim, kernel size = 1, stride = 1\n",
    "        self.emb_ld = tf.compat.v1.layers.Conv1D(dim,1,_scope=_scope+_name+'/emb_ld' ) #conv1d_2\n",
    "\n",
    "        self.project_d = tf.compat.v1.layers.Conv1D(dim,1,_scope=_scope+_name+'/proj_d' ) #conv1d_1\n",
    "        self.project_ld = tf.compat.v1.layers.Conv1D(dim,1,_scope=_scope+_name+'/proj_ld' ) #conv1d_3\n",
    "        self.project_query = tf.compat.v1.layers.Dense(dim,_scope=_scope+_name+'/proj_q' ) # fully connected layer, activation is linear\n",
    "        self.project_ref = tf.compat.v1.layers.Conv1D(dim,1,_scope=_scope+_name+'/proj_ref' ) #conv1d_4\n",
    "\n",
    "\n",
    "        self.C = C  # tanh exploration parameter\n",
    "        self.tanh = tf.nn.tanh      # activation function hyperbolique tangente (output in ]-1,1[\n",
    "\n",
    "    def __call__(self, query, ref, env):\n",
    "        \"\"\"\n",
    "        This function gets a query tensor and ref tensor and returns the logit op.\n",
    "        Args: \n",
    "            query: is the hidden state of the decoder at the current\n",
    "                time step. [batch_size x dim]\n",
    "            ref: the set of hidden states from the encoder. \n",
    "                [batch_size x max_time x dim]\n",
    "\n",
    "        Returns:\n",
    "            e: convolved ref with shape [batch_size x max_time x dim]\n",
    "            logits: [batch_size x max_time]\n",
    "        \"\"\"\n",
    "        # get the current demand and load values from environment\n",
    "        demand = env.demand\n",
    "        load = env.load\n",
    "        max_time = tf.shape(input=demand)[1]\n",
    "\n",
    "\n",
    "        # embed demand and project it\n",
    "        # emb_d:[batch_size x max_time x dim ]\n",
    "        emb_d = self.emb_d(tf.expand_dims(demand,2))\n",
    "        # d:[batch_size x max_time x dim ]\n",
    "        d = self.project_d(emb_d)\n",
    "\n",
    "        # embed load - demand\n",
    "        # emb_ld:[batch_size*beam_width x max_time x hidden_dim]\n",
    "        emb_ld = self.emb_ld(tf.expand_dims(tf.tile(tf.expand_dims(load,1),[1,max_time])-\n",
    "                                              demand,2))\n",
    "        # ld:[batch_size*beam_width x hidden_dim x max_time ] \n",
    "        ld = self.project_ld(emb_ld)\n",
    "\n",
    "        # expanded_q,e: [batch_size x max_time x dim]\n",
    "        e = self.project_ref(ref)\n",
    "        q = self.project_query(query) #[batch_size x dim]\n",
    "        expanded_q = tf.tile(tf.expand_dims(q,1),[1,max_time,1])\n",
    "\n",
    "        # v_view:[batch_size x dim x 1]\n",
    "        v_view = tf.tile( self.v, [tf.shape(input=e)[0],1,1]) \n",
    "        \n",
    "        # u : [batch_size x max_time x dim] * [batch_size x dim x 1] = \n",
    "        #       [batch_size x max_time]\n",
    "        u = tf.squeeze(tf.matmul(self.tanh(expanded_q + e + d + ld), v_view),2)\n",
    "\n",
    "        if self.use_tanh:\n",
    "            logits = self.C * self.tanh(u)\n",
    "        else:\n",
    "            logits = u  \n",
    "\n",
    "        return e, logits\n",
    "\n",
    "\n",
    "class AttentionVRPCritic(object):\n",
    "    \"\"\"A generic attention module for the attention in vrp model\"\"\"\n",
    "    def __init__(self, dim, use_tanh=False, C=10,_name='Attention',_scope=''):\n",
    "\n",
    "        self.use_tanh = use_tanh\n",
    "        self._scope = _scope\n",
    "\n",
    "        with tf.compat.v1.variable_scope(_scope+_name):\n",
    "            # self.v: is a variable with shape [1 x dim]\n",
    "            self.v = tf.compat.v1.get_variable('v',[1,dim],\n",
    "                       initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "            self.v = tf.expand_dims(self.v,2)\n",
    "            \n",
    "        self.emb_d = tf.compat.v1.layers.Conv1D(dim,1,_scope=_scope+_name +'/emb_d') #conv1d\n",
    "        self.project_d = tf.compat.v1.layers.Conv1D(dim,1,_scope=_scope+_name +'/proj_d') #conv1d_1\n",
    "        \n",
    "        self.project_query = tf.compat.v1.layers.Dense(dim,_scope=_scope+_name +'/proj_q') #\n",
    "        self.project_ref = tf.compat.v1.layers.Conv1D(dim,1,_scope=_scope+_name +'/proj_e') #conv1d_2\n",
    "\n",
    "        self.C = C  # tanh exploration parameter\n",
    "        self.tanh = tf.nn.tanh\n",
    "        \n",
    "    def __call__(self, query, ref, env):\n",
    "        \"\"\"\n",
    "        This function gets a query tensor and ref rensor and returns the logit op.\n",
    "        Args: \n",
    "            query: is the hidden state of the decoder at the current\n",
    "                time step. [batch_size x dim]\n",
    "            ref: the set of hidden states from the encoder. \n",
    "                [batch_size x max_time x dim]\n",
    "\n",
    "            env: keeps demand ond load values and help decoding. Also it includes mask.\n",
    "                env.mask: a matrix used for masking the logits and glimpses. It is with shape\n",
    "                         [batch_size x max_time]. Zeros in this matrix means not-masked nodes. Any \n",
    "                         positive number in this mask means that the node cannot be selected as next \n",
    "                         decision point.\n",
    "                env.demands: a list of demands which changes over time.\n",
    "\n",
    "        Returns:\n",
    "            e: convolved ref with shape [batch_size x max_time x dim]\n",
    "            logits: [batch_size x max_time]\n",
    "        \"\"\"\n",
    "        # we need the first demand value for the critic\n",
    "        demand = env.input_data[:,:,-1]\n",
    "        max_time = tf.shape(input=demand)[1]\n",
    "\n",
    "        # embed demand and project it\n",
    "        # emb_d:[batch_size x max_time x dim ]\n",
    "        emb_d = self.emb_d(tf.expand_dims(demand,2))\n",
    "        # d:[batch_size x max_time x dim ]\n",
    "        d = self.project_d(emb_d)\n",
    "\n",
    "\n",
    "        # expanded_q,e: [batch_size x max_time x dim]\n",
    "        e = self.project_ref(ref)\n",
    "        q = self.project_query(query) #[batch_size x dim]\n",
    "        expanded_q = tf.tile(tf.expand_dims(q,1),[1,max_time,1])\n",
    "\n",
    "        # v_view:[batch_size x dim x 1]\n",
    "        v_view = tf.tile( self.v, [tf.shape(input=e)[0],1,1]) \n",
    "        \n",
    "        # u : [batch_size x max_time x dim] * [batch_size x dim x 1] = \n",
    "        #       [batch_size x max_time]\n",
    "        u = tf.squeeze(tf.matmul(self.tanh(expanded_q + e + d), v_view),2)\n",
    "\n",
    "        if self.use_tanh:\n",
    "            logits = self.C * self.tanh(u)\n",
    "        else:\n",
    "            logits = u  \n",
    "\n",
    "        return e, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbd95639-bbf5-4ed1-a7d0-00893464abef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    \n",
    "def load_task_specific_components(task,ups):\n",
    "    '''\n",
    "    This function load task-specific libraries\n",
    "    '''\n",
    "    if task == 'vrp':\n",
    "        if ups:\n",
    "            from UPS.vrp_ups_utils import DataGenerator,Env,reward_func\n",
    "            from UPS.vrp_ups_attention import AttentionVRP_UPS_Actor, AttentionVRP_UPS_Critic\n",
    "\n",
    "            AttentionActor = AttentionVRP_UPS_Actor\n",
    "            AttentionCritic = AttentionVRP_UPS_Critic\n",
    "\n",
    "        else:\n",
    "            #from VRP.vrp_utils import DataGenerator,Env,reward_func\n",
    "           \n",
    "            #DataGenerator = DataGenerator\n",
    "            #Env = Env\n",
    "            #reward_func = reward_func\n",
    "            \n",
    "            #from VRP.vrp_attention import AttentionVRPActor,AttentionVRPCritic\n",
    "\n",
    "            AttentionActor = AttentionVRPActor\n",
    "            AttentionCritic = AttentionVRPCritic\n",
    "\n",
    "    elif task == 'vrptw':\n",
    "        if ups:\n",
    "            from UPS.vrptw_ups_utils import DataGenerator,Env,reward_func\n",
    "            from UPS.vrptw_ups_attention import AttentionVRPTW_UPS_Actor, AttentionVRPTW_UPS_Critic\n",
    "\n",
    "            AttentionActor = AttentionVRPTW_UPS_Actor\n",
    "            AttentionCritic = AttentionVRPTW_UPS_Critic\n",
    "        else:\n",
    "            from VRPTW.vrptw_utils import DataGenerator,Env,reward_func\n",
    "            from VRPTW.vrptw_attention import AttentionVRPTWActor, AttentionVRPTWCritic\n",
    "\n",
    "            AttentionActor = AttentionVRPTWActor\n",
    "            AttentionCritic = AttentionVRPTWCritic\n",
    "\n",
    "    else:\n",
    "        raise Exception('Task is not implemented')\n",
    "\n",
    "    return AttentionActor, AttentionCritic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04130236",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_task_specific_eval(task):\n",
    "    \"\"\"\n",
    "    Load taks specific, dependign of tw or not\n",
    "    \"\"\"\n",
    "    if task == 'vrp':\n",
    "        from evaluation.eval_VRP import eval_google_or,eval_Clarke_Wright\n",
    "\n",
    "        return [(eval_google_or.EvalGoogleOR,'or_tools'), (eval_Clarke_Wright.EvalClarkeWright,'Clarke_Wright')]\n",
    "\n",
    "    elif task == 'vrptw':\n",
    "        from evaluation.eval_VRPTW import eval_tw_google_or,eval_I1_heuristics\n",
    "\n",
    "        return [(eval_tw_google_or.EvalTWGoogleOR,'or_tools_tw'),(eval_I1_heuristics.EvalI1Heuristics,'I1_heuristic')]\n",
    "\n",
    "    else:\n",
    "        raise Exception('Task is not implemented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "668cda45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(args, prt):\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.compat.v1.Session(config=config)\n",
    "    \n",
    "    # load task specific classes\n",
    "    AttentionActor, AttentionCritic = \\\n",
    "        load_task_specific_components(args['task_name'],args['ups'])\n",
    "\n",
    "    dataGen = DataGenerator(args)\n",
    "    dataGen.reset()\n",
    "    env = Env(args)\n",
    "    # create an RL agent\n",
    "    agent = RLAgent(args,\n",
    "                    prt,\n",
    "                    env,\n",
    "                    dataGen,\n",
    "                    reward_func,\n",
    "                    AttentionActor,\n",
    "                    AttentionCritic,\n",
    "                    is_train=args['is_train'])\n",
    "    agent.Initialize(sess)\n",
    "\n",
    "    # train or evaluate\n",
    "    prev_actor_loss, prev_critic_loss = float('Inf'), float('Inf')\n",
    "    actor_eps, critic_eps = 1e-2, 1e-2\n",
    "    start_time = time.time()\n",
    "    convergence_counter = 0\n",
    "    al_file = open(args['log_dir']+\"/actorLoss.txt\", \"w\")\n",
    "    cl_file = open(args['log_dir']+\"/criticLoss.txt\", \"w\")\n",
    "    r_file = open(args['log_dir']+\"/reward.txt\", \"w\")\n",
    "\n",
    "    if args['is_train']:\n",
    "        prt.print_out('Training started ...')\n",
    "        train_time_beg = time.time()\n",
    "        for step in range(args['n_train']):\n",
    "            summary = agent.run_train_step()\n",
    "            _, _ , actor_loss_val, critic_loss_val, actor_gra_and_var_val, critic_gra_and_var_val,\\\n",
    "                R_val, v_val, logprobs_val,probs_val, actions_val, idxs_val= summary\n",
    "\n",
    "            curr_actor_loss = np.mean(actor_loss_val)\n",
    "            curr_critic_loss = np.mean(critic_loss_val)\n",
    "            al_file.write( str(actor_loss_val) + '\\n')\n",
    "            cl_file.write(str(critic_loss_val) + '\\n')\n",
    "            r_file.write(str(np.mean(R_val)) + '\\n')\n",
    "\n",
    "            if abs(prev_actor_loss - curr_actor_loss) < actor_eps \\\n",
    "                and abs(prev_critic_loss - curr_critic_loss) < critic_eps:\n",
    "                convergence_counter += 1\n",
    "            else:\n",
    "                convergence_counter = 0\n",
    "            if convergence_counter == 10:\n",
    "                prt.print_out('Converged at step {}'\\\n",
    "                      .format(step))\n",
    "                train_time_end = time.time()-train_time_beg\n",
    "                prt.print_out('Train Step: {} -- Time: {} -- Train reward: {} -- Value: {}'\\\n",
    "                      .format(step,time.strftime(\"%H:%M:%S\", time.gmtime(\\\n",
    "                        train_time_end)),np.mean(R_val),np.mean(v_val)))\n",
    "                prt.print_out('    actor loss: {} -- critic loss: {}'\\\n",
    "                      .format(curr_actor_loss,curr_critic_loss))\n",
    "                break\n",
    "\n",
    "            if step%args['save_interval'] == 0:\n",
    "                agent.saver.save(sess,args['model_dir']+'/model.ckpt', global_step=step)\n",
    "\n",
    "            if step%args['log_interval'] == 0:\n",
    "                train_time_end = time.time()-train_time_beg\n",
    "                prt.print_out('Train Step: {} -- Time: {} -- Embedding Time {} -- Train reward: {} -- Value: {}'\\\n",
    "                      .format(step,time.strftime(\"%H:%M:%S\", time.gmtime(\\\n",
    "                        train_time_end)),time.strftime(\"%H:%M:%S\", time.gmtime(\\\n",
    "                        agent.embedder_model.total_time)),np.mean(R_val),np.mean(v_val)))\n",
    "                prt.print_out('    actor loss: {} -- critic loss: {}'\\\n",
    "                      .format(curr_actor_loss, curr_critic_loss))\n",
    "\n",
    "                train_time_beg = time.time()\n",
    "                agent.embedder_model.total_time = 0\n",
    "            if step%args['test_interval'] == 0:\n",
    "                agent.inference(args['infer_type'])\n",
    "            prev_actor_loss = curr_actor_loss\n",
    "            prev_critic_loss = curr_critic_loss\n",
    "\n",
    "        # Save the model at the end of the training\n",
    "        agent.saver.save(sess,args['model_dir']+'/model.ckpt', global_step=step)\n",
    "\n",
    "    else: # inference\n",
    "        prt.print_out('Evaluation started ...')\n",
    "        agent.inference(args['infer_type'])\n",
    "\n",
    "        all_evaluator = load_task_specific_eval(args['task_name'])\n",
    "\n",
    "        # perform the evaluation\n",
    "        list_eval = ['beam_search'] #['greedy','beam_search']\n",
    "        for eval_tuple in all_evaluator:\n",
    "            list_eval.append(eval_tuple[1])\n",
    "\n",
    "            object_eval = eval_tuple[0](args,env,prt,args['min_trucks'])\n",
    "            object_eval.perform_routing()\n",
    "        #\n",
    "        benchmark_object = benchmark.Benchmark(args,env,prt)\n",
    "        # list_eval.remove('Clarke_Wright')\n",
    "        # #list_eval.remove('I1_heuristic')\n",
    "        benchmark_object.perform_benchmark(list_eval=list_eval)\n",
    "\n",
    "    prt.print_out('Total time is {}'.format(time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_time))))\n",
    "    al_file.close()\n",
    "    cl_file.close()\n",
    "    r_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db56ab94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor_net_lr: 0.0001\n",
      "agent_type: attention\n",
      "batch_size: 6\n",
      "beam_width: 5\n",
      "capacity: 20\n",
      "critic_net_lr: 0.0001\n",
      "data_dir: ./data\n",
      "decode_len: 20\n",
      "demand_max: 9\n",
      "disable_tqdm: True\n",
      "dropout: 0.1\n",
      "embedding_dim: 30\n",
      "embedding_graph: 2\n",
      "entropy_coeff: 0.0\n",
      "forget_bias: 1.0\n",
      "gpu: 3\n",
      "hidden_dim: 128\n",
      "infer_type: batch\n",
      "input_dim: 3\n",
      "is_train: True\n",
      "load_path: \n",
      "log_dir: logs/vrp10-2021-12-14_12-50-06\n",
      "log_interval: 200\n",
      "mask_glimpses: True\n",
      "mask_pointer: True\n",
      "max_grad_norm: 2.0\n",
      "min_trucks: False\n",
      "model_dir: logs/vrp10-2021-12-14_12-50-06\\model\n",
      "n_cust: 10\n",
      "n_glimpses: 0\n",
      "n_nodes: 11\n",
      "n_process_blocks: 3\n",
      "n_train: 10\n",
      "random_seed: 24601\n",
      "rnn_layers: 1\n",
      "save_interval: 1000\n",
      "stdout_print: True\n",
      "tanh_exploration: 10.0\n",
      "task: vrp10\n",
      "task_name: vrp\n",
      "test_interval: 200\n",
      "test_size: 1000\n",
      "ups: False\n",
      "use_tanh: False\n",
      "# Set random seed to 24601\n",
      "Created train iterator.\n",
      "Loading dataset for vrp-size-1000-len-11-test.txt...\n",
      "WARNING:tensorflow:From C:/Users/wangmin/AppData/Local/Temp/xpython_932/4143687115.py:175: BasicLSTMCell.__init__ (from tensorflow.python.keras.layers.legacy_rnn.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From C:/Users/wangmin/AppData/Local/Temp/xpython_932/4143687115.py:180: MultiRNNCell.__init__ (from tensorflow.python.keras.layers.legacy_rnn.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #test()\n",
    "    #assert False\n",
    "    args, prt = ParseParams()\n",
    "    args['is_train'] = True \n",
    "    # args['infer_type'] = 'single' \n",
    "    \n",
    "    args['test_size'] = 1000\n",
    "   #  args['log_dir'] = \"/Users/jpoullet/Documents/MIT/Thesis/ML6867_project/VRP-RL/logs/vrp20-2019-12-05_09-28-11/\"\n",
    "    # args['load_path'] = \"/Users/jpoullet/Documents/MIT/Thesis/ML6867_project/VRP-RL/logs/vrp50-NbTruck/model/\"\n",
    "\n",
    "    # args['data_dir'] = \"drive/My Drive/VRP-RL/data\"\n",
    "    # args['log_dir'] = \"drive/My Drive/VRP-RL/logs\"\n",
    "    # args['log_dir'] = \"{}/{}-{}\".format(args['log_dir'],args['task'], utils.get_time())\n",
    "    # print(args['log_dir'])\n",
    "    # args['model_dir'] = os.path.join(args['log_dir'],'model')\n",
    "    #\n",
    "    # args['load_path'] = \"drive/My Drive/VRP-RL/logs/vrptw50-2019-11-25_01-28-09/model/\"\n",
    "    # print(args['model_dir'])\n",
    "    # # file to write the stdout\n",
    "    # try:\n",
    "    #     os.makedirs(args['log_dir'])\n",
    "    #     os.makedirs(args['model_dir'])\n",
    "    # except:\n",
    "    #     pass\n",
    "    #\n",
    "    # # create a print handler\n",
    "    # out_file = open(os.path.join(args['log_dir'], 'results.txt'),'w+')\n",
    "    # prt = utils.printOut(out_file,args['stdout_print'])\n",
    "\n",
    "    # Random\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    random_seed = args['random_seed']\n",
    "    if random_seed is not None and random_seed > 0:\n",
    "        prt.print_out(\"# Set random seed to %d\" % random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        tf.random.set_seed(random_seed)\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "\n",
    "    main(args, prt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547757b4-5571-475f-a3b1-5e4865e823e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
