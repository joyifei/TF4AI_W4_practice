{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40b58181-a7e3-41d2-938a-c9024c42ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import sys\n",
    "from configs import ParseParams\n",
    "from DataGenerator import DataGenerator\n",
    "from env import Env\n",
    "#from Attention import AttentionVRPCritic, AttentionVRPActor\n",
    "from embedder import FullGraphEmbedding\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8311133c-1a97-4dfb-ad9e-416006634624",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionVRPActor(object):\n",
    "    \"\"\"A generic attention module for the attention in vrp model\"\"\"\n",
    "    def __init__(self, dim, use_tanh=False, C=10,_name='Attention',_scope=''):\n",
    "        self.use_tanh = use_tanh\n",
    "        self._scope = _scope\n",
    "\n",
    "        with tf.compat.v1.variable_scope(_scope+_name):\n",
    "            # self.v: is a variable with shape [1 x dim]\n",
    "            self.v = tf.compat.v1.get_variable('v',[1,dim],\n",
    "                       initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "            self.v = tf.expand_dims(self.v,2)\n",
    "\n",
    "        self.emb_d = tf.compat.v1.layers.Conv1D(dim,1,_scope=_scope+_name+'/emb_d' ) #conv1d of kernel size = dim, stride = 1\n",
    "                                                                                     # here should be filters = dim, kernel size = 1, stride = 1\n",
    "        self.emb_ld = tf.compat.v1.layers.Conv1D(dim,1,_scope=_scope+_name+'/emb_ld' ) #conv1d_2\n",
    "\n",
    "        self.project_d = tf.compat.v1.layers.Conv1D(dim,1,_scope=_scope+_name+'/proj_d' ) #conv1d_1\n",
    "        self.project_ld = tf.compat.v1.layers.Conv1D(dim,1,_scope=_scope+_name+'/proj_ld' ) #conv1d_3\n",
    "        self.project_query = tf.compat.v1.layers.Dense(dim,_scope=_scope+_name+'/proj_q' ) # fully connected layer, activation is linear\n",
    "        self.project_ref = tf.compat.v1.layers.Conv1D(dim,1,_scope=_scope+_name+'/proj_ref' ) #conv1d_4\n",
    "\n",
    "\n",
    "        self.C = C  # tanh exploration parameter\n",
    "        self.tanh = tf.nn.tanh      # activation function hyperbolique tangente (output in ]-1,1[\n",
    "\n",
    "    def __call__(self, query, ref, env):\n",
    "        \"\"\"\n",
    "        This function gets a query tensor and ref tensor and returns the logit op.\n",
    "        Args: \n",
    "            query: is the hidden state of the decoder at the current\n",
    "                time step. [batch_size x dim]\n",
    "            ref: the set of hidden states from the encoder. \n",
    "                [batch_size x max_time x dim]\n",
    "\n",
    "        Returns:\n",
    "            e: convolved ref with shape [batch_size x max_time x dim]\n",
    "            logits: [batch_size x max_time]\n",
    "        \"\"\"\n",
    "        # get the current demand and load values from environment\n",
    "        demand = env.demand\n",
    "        load = env.load\n",
    "        max_time = tf.shape(input=demand)[1]\n",
    "\n",
    "\n",
    "        # embed demand and project it\n",
    "        # emb_d:[batch_size x max_time x dim ]\n",
    "        emb_d = self.emb_d(tf.expand_dims(demand,2))\n",
    "        # d:[batch_size x max_time x dim ]\n",
    "        d = self.project_d(emb_d)\n",
    "\n",
    "        # embed load - demand\n",
    "        # emb_ld:[batch_size*beam_width x max_time x hidden_dim]\n",
    "        emb_ld = self.emb_ld(tf.expand_dims(tf.tile(tf.expand_dims(load,1),[1,max_time])-\n",
    "                                              demand,2))\n",
    "        # ld:[batch_size*beam_width x hidden_dim x max_time ] \n",
    "        ld = self.project_ld(emb_ld)\n",
    "\n",
    "        # expanded_q,e: [batch_size x max_time x dim]\n",
    "        e = self.project_ref(ref)\n",
    "        q = self.project_query(query) #[batch_size x dim]\n",
    "        expanded_q = tf.tile(tf.expand_dims(q,1),[1,max_time,1])\n",
    "\n",
    "        # v_view:[batch_size x dim x 1]\n",
    "        v_view = tf.tile( self.v, [tf.shape(input=e)[0],1,1]) \n",
    "        \n",
    "        # u : [batch_size x max_time x dim] * [batch_size x dim x 1] = \n",
    "        #       [batch_size x max_time]\n",
    "        u = tf.squeeze(tf.matmul(self.tanh(expanded_q + e + d + ld), v_view),2)\n",
    "\n",
    "        if self.use_tanh:\n",
    "            logits = self.C * self.tanh(u)\n",
    "        else:\n",
    "            logits = u  \n",
    "\n",
    "        return e, logits\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46d9d450-9728-4e2b-bf50-774ce0bb89a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "class AttentionVRPCritic(object):\n",
    "    \"\"\"A generic attention module for the attention in vrp model\"\"\"\n",
    "    def __init__(self, dim, use_tanh=False, C=10,_name='Attention',_scope=''):\n",
    "\n",
    "        self.use_tanh = use_tanh\n",
    "        self._scope = _scope\n",
    "\n",
    "        with tf.compat.v1.variable_scope(_scope+_name):\n",
    "            # self.v: is a variable with shape [1 x dim]\n",
    "            self.v = tf.compat.v1.get_variable('v',[1,dim],\n",
    "                       initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "            self.v = tf.expand_dims(self.v,2)\n",
    "            \n",
    "        self.emb_d = tf.compat.v1.layers.Conv1D(dim,1,_scope=_scope+_name +'/emb_d') #conv1d\n",
    "        self.project_d = tf.compat.v1.layers.Conv1D(dim,1,_scope=_scope+_name +'/proj_d') #conv1d_1\n",
    "        \n",
    "        self.project_query = tf.compat.v1.layers.Dense(dim,_scope=_scope+_name +'/proj_q') #\n",
    "        self.project_ref = tf.compat.v1.layers.Conv1D(dim,1,_scope=_scope+_name +'/proj_e') #conv1d_2\n",
    "\n",
    "        self.C = C  # tanh exploration parameter\n",
    "        self.tanh = tf.nn.tanh\n",
    "        \n",
    "    def __call__(self, query, ref, env):\n",
    "        \"\"\"\n",
    "        This function gets a query tensor and ref rensor and returns the logit op.\n",
    "        Args: \n",
    "            query: is the hidden state of the decoder at the current\n",
    "                time step. [batch_size x dim]\n",
    "            ref: the set of hidden states from the encoder. \n",
    "                [batch_size x max_time x dim]\n",
    "\n",
    "            env: keeps demand ond load values and help decoding. Also it includes mask.\n",
    "                env.mask: a matrix used for masking the logits and glimpses. It is with shape\n",
    "                         [batch_size x max_time]. Zeros in this matrix means not-masked nodes. Any \n",
    "                         positive number in this mask means that the node cannot be selected as next \n",
    "                         decision point.\n",
    "                env.demands: a list of demands which changes over time.\n",
    "\n",
    "        Returns:\n",
    "            e: convolved ref with shape [batch_size x max_time x dim]\n",
    "            logits: [batch_size x max_time]\n",
    "        \"\"\"\n",
    "        # we need the first demand value for the critic\n",
    "        demand = env.input_data[:,:,-1]\n",
    "        max_time = tf.shape(input=demand)[1]\n",
    "\n",
    "        # embed demand and project it\n",
    "        # emb_d:[batch_size x max_time x dim ]\n",
    "        emb_d = self.emb_d(tf.expand_dims(demand,2))\n",
    "        # d:[batch_size x max_time x dim ]\n",
    "        d = self.project_d(emb_d)\n",
    "\n",
    "\n",
    "        # expanded_q,e: [batch_size x max_time x dim]\n",
    "        e = self.project_ref(ref)\n",
    "        q = self.project_query(query) #[batch_size x dim]\n",
    "        expanded_q = tf.tile(tf.expand_dims(q,1),[1,max_time,1])\n",
    "\n",
    "        # v_view:[batch_size x dim x 1]\n",
    "        v_view = tf.tile( self.v, [tf.shape(input=e)[0],1,1]) \n",
    "        \n",
    "        # u : [batch_size x max_time x dim] * [batch_size x dim x 1] = \n",
    "        #       [batch_size x max_time]\n",
    "        u = tf.squeeze(tf.matmul(self.tanh(expanded_q + e + d), v_view),2)\n",
    "\n",
    "        if self.use_tanh:\n",
    "            logits = self.C * self.tanh(u)\n",
    "        else:\n",
    "            logits = u  \n",
    "\n",
    "        return e, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e578dbb3-e3e8-4e80-bbe0-249774e3ff2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecodeStep(object):\n",
    "    '''\n",
    "    Base class of the decoding (without RNN)\n",
    "    '''\n",
    "    def __init__(self, \n",
    "            ClAttention,\n",
    "            args,\n",
    "            hidden_dim,\n",
    "            use_tanh=False,\n",
    "            tanh_exploration=10.,\n",
    "            n_glimpses=0,\n",
    "            mask_glimpses=True,\n",
    "            mask_pointer=True,\n",
    "            _scope=''):\n",
    "        '''\n",
    "        This class does one-step of decoding.\n",
    "        Inputs:\n",
    "            ClAttention:    the class which is used for attention\n",
    "            hidden_dim:     hidden dimension of RNN\n",
    "            use_tanh:       whether to use tanh exploration or not\n",
    "            tanh_exploration: parameter for tanh exploration\n",
    "            n_glimpses:     number of glimpses\n",
    "            mask_glimpses:  whether to use masking for the glimpses or not\n",
    "            mask_pointer:   whether to use masking for the glimpses or not\n",
    "            _scope:         variable scope\n",
    "        '''\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.use_tanh = use_tanh\n",
    "        self.tanh_exploration = tanh_exploration\n",
    "        self.n_glimpses = n_glimpses\n",
    "        self.mask_glimpses = mask_glimpses\n",
    "        self.mask_pointer = mask_pointer\n",
    "        self._scope = _scope\n",
    "        self.BIGNUMBER = 100000.\n",
    "\n",
    "\n",
    "        # create glimpse and attention instances as well as tf.variables.\n",
    "        ## create a list of class instances\n",
    "        self.glimpses = [None for _ in range(self.n_glimpses)]\n",
    "        for i in range(self.n_glimpses):\n",
    "            self.glimpses[i] = ClAttention(hidden_dim, \n",
    "                use_tanh=False,\n",
    "                _scope=self._scope,\n",
    "                _name=\"Glimpse\"+str(i))\n",
    "            \n",
    "        # build TF variables required for pointer\n",
    "        self.pointer = ClAttention(hidden_dim, \n",
    "            use_tanh=use_tanh, \n",
    "            C=tanh_exploration,\n",
    "            _scope=self._scope,\n",
    "            _name=\"Decoder/Attention\")\n",
    "\n",
    "    def get_logit_op(self,\n",
    "                     decoder_inp,\n",
    "                     context,\n",
    "                     Env,\n",
    "                    *args,\n",
    "                    **kwargs):\n",
    "        \"\"\"\n",
    "        For a given input to deocoder, returns the logit op.\n",
    "        Input:\n",
    "            decoder_inp: it is the input problem with dimensions [batch_size x dim].\n",
    "                        Usually, it is the embedded problem with dim = embedding_dim.\n",
    "            context: the context vetor from the encoder. It is usually the output of rnn with\n",
    "                      shape [batch_size x max_time x dim]\n",
    "            Env: an instance of the environment. It should have:\n",
    "                Env.mask: a matrix used for masking the logits and glimpses. It is with shape\n",
    "                         [batch_size x max_time]. Zeros in this matrix means not-masked nodes. Any \n",
    "                         positive number in this mask means that the node cannot be selected as \n",
    "                         the next decision point.\n",
    "        Returns:\n",
    "            logit: the logits which will used by decoder for producing a solution. It has shape\n",
    "            [batch_size x max_time].\n",
    "        \"\"\"\n",
    "\n",
    "        # glimpses\n",
    "        for i in range(self.n_glimpses):\n",
    "            # ref: [batch_size x max_time x hidden_dim], logit : [batch_size x max_time]\n",
    "            ref, logit = self.glimpses[i](decoder_inp, context,Env)\n",
    "            if self.mask_glimpses:\n",
    "                logit -= self.BIGNUMBER* Env.mask\n",
    "            # prob: [batch_size x max_time\n",
    "            prob = tf.nn.softmax(logit)\n",
    "            # decoder_inp : [batch_size x 1 x max_time ] * [batch_size x max_time x hidden_dim] -> \n",
    "            #[batch_size x hidden_dim ]\n",
    "            decoder_inp = tf.squeeze(tf.matmul( tf.expand_dims(prob,1),ref) ,1)\n",
    "\n",
    "        # attention\n",
    "        _, logit = self.pointer(decoder_inp,context,Env)\n",
    "        if self.mask_pointer:\n",
    "            logit -= self.BIGNUMBER* Env.mask\n",
    "\n",
    "        return logit , None\n",
    "\n",
    "    def step(self,\n",
    "            decoder_inp,\n",
    "            context,\n",
    "            Env,\n",
    "            decoder_state=None,\n",
    "            *args,\n",
    "            **kwargs):\n",
    "        '''\n",
    "        get logits and probs at a given decoding step.\n",
    "        Inputs:\n",
    "            decoder_input: Input of the decoding step with shape [batch_size x embedding_dim]\n",
    "            context: context vector to use in attention\n",
    "            Env: an instance of the environment\n",
    "            decoder_state: The state of the LSTM cell. It can be None when we use a decoder without \n",
    "                LSTM cell.\n",
    "        Returns:\n",
    "            logit: logits with shape [batch_size x max_time]\n",
    "            prob: probabilities for the next location visit with shape of [batch_size x max_time]\n",
    "            logprob: log of probabilities\n",
    "            decoder_state: updated state of the LSTM cell\n",
    "        '''\n",
    "\n",
    "        logit, decoder_state = self.get_logit_op(\n",
    "                     decoder_inp,\n",
    "                     context,\n",
    "                     Env, \n",
    "                     decoder_state)\n",
    "\n",
    "        logprob = tf.nn.log_softmax(logit)\n",
    "        prob = tf.exp(logprob)\n",
    "        #prob has shape(batch_num, node_number ),  here is (batch_number, 11),  for each batch,  the one node's value will be close to 1, which\n",
    "        #is the stop this agent believe should visit next\n",
    "        #next step,  figure out what to do next with this prob\n",
    "        return logit, prob, logprob, decoder_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c1008a9-bf4f-44dd-b697-db23d6de4121",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNDecodeStep(DecodeStep):\n",
    "    '''\n",
    "    Decodes the sequence. It keeps the decoding history in a RNN.\n",
    "    '''\n",
    "    def __init__(self, \n",
    "            ClAttention,\n",
    "            args,\n",
    "            hidden_dim,\n",
    "            use_tanh=False,\n",
    "            tanh_exploration=10.,\n",
    "            n_glimpses=0,\n",
    "            mask_glimpses=True,\n",
    "            mask_pointer=True,\n",
    "            forget_bias=1.0,\n",
    "            rnn_layers=1,\n",
    "            _scope=''):\n",
    "\n",
    "        '''\n",
    "        This class does one-step of decoding which uses RNN for storing the sequence info.\n",
    "        Inputs:\n",
    "            ClAttention:    the class which is used for attention\n",
    "            hidden_dim:     hidden dimension of RNN\n",
    "            use_tanh:       whether to use tanh exploration or not\n",
    "            tanh_exploration: parameter for tanh exploration\n",
    "            n_glimpses:     number of glimpses\n",
    "            mask_glimpses:  whether to use masking for the glimpses or not\n",
    "            mask_pointer:   whether to use masking for the glimpses or not\n",
    "            forget_bias:    forget bias of LSTM\n",
    "            rnn_layers:     number of LSTM layers\n",
    "            _scope:         variable scope\n",
    "\n",
    "        '''\n",
    "\n",
    "        super(RNNDecodeStep,self).__init__(ClAttention,\n",
    "                                        args,\n",
    "                                        hidden_dim,\n",
    "                                        use_tanh=use_tanh,\n",
    "                                        tanh_exploration=tanh_exploration,\n",
    "                                        n_glimpses=n_glimpses,\n",
    "                                        mask_glimpses=mask_glimpses,\n",
    "                                        mask_pointer=mask_pointer,\n",
    "                                        _scope=_scope)\n",
    "        self.forget_bias = forget_bias\n",
    "        self.rnn_layers = rnn_layers     \n",
    "#         self.dropout = tf.placeholder(tf.float32,name='decoder_rnn_dropout')\n",
    "\n",
    "        # build a multilayer LSTM cell\n",
    "        single_cell = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(hidden_dim, \n",
    "            forget_bias=forget_bias)\n",
    "        self.dropout = tf.constant( args['dropout'] )\n",
    "        single_cell = tf.compat.v1.nn.rnn_cell.DropoutWrapper(\n",
    "                cell=single_cell, input_keep_prob=(1.0 - self.dropout))\n",
    "        self.cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell([single_cell] * rnn_layers)\n",
    "\n",
    "    def get_logit_op(self,\n",
    "                    decoder_inp,\n",
    "                    context,\n",
    "                    Env,\n",
    "                    decoder_state,\n",
    "                    *args,\n",
    "                    **kwargs):\n",
    "        \"\"\"\n",
    "        For a given input to decoder, returns the logit op and new decoder_state.\n",
    "        Input:\n",
    "            decoder_inp: it is the input problem with dimensions [batch_size x dim].\n",
    "                        Usually, it is the embedded problem with dim = embedding_dim.\n",
    "            context: the context vetor from the encoder. It is usually the output of rnn with\n",
    "                      shape [batch_size x max_time x dim]\n",
    "            Env: an instance of the environment. It should have:\n",
    "                Env.mask: a matrix used for masking the logits and glimpses. It is with shape\n",
    "                         [batch_size x max_time]. Zeros in this matrix means not-masked nodes. Any \n",
    "                         positive number in this mask means that the node cannot be selected as \n",
    "                         the next decision point.\n",
    "            decoder_state: The state as a list of size rnn_layers, and each element is a\n",
    "                    LSTMStateTuples with  x 2 tensors with dimension of [batch_size x hidden_dim].\n",
    "                    The first one corresponds to c and the second one is h.\n",
    "        Returns:\n",
    "            logit: the logits which will used by decoder for producing a solution. It has shape\n",
    "                    [batch_size x max_time].\n",
    "            decoder_state: the update decoder state.\n",
    "        \"\"\"\n",
    "\n",
    "        # decoder_inp = tf.reshape(decoder_inp,[-1,1,self.hidden_dim])\n",
    "        _ , decoder_state = tf.compat.v1.nn.dynamic_rnn(self.cell,\n",
    "                                              decoder_inp,\n",
    "                                              initial_state=decoder_state,\n",
    "                                              scope=self._scope+'Decoder/LSTM/rnn')\n",
    "        hy = decoder_state[-1].h\n",
    "\n",
    "        # glimpses\n",
    "        for i in range(self.n_glimpses):\n",
    "            # ref: [batch_size x max_time x hidden_dim], logit : [batch_size x max_time]\n",
    "            ref, logit = self.glimpses[i](hy,context,Env)\n",
    "            if self.mask_glimpses:\n",
    "                logit -= self.BIGNUMBER* Env.mask\n",
    "            prob = tf.nn.softmax(logit)\n",
    "            \n",
    "            # hy : [batch_size x 1 x max_time ] * [batch_size x max_time x hidden_dim] -> \n",
    "            #[batch_size x hidden_dim ]\n",
    "            hy = tf.squeeze(tf.matmul( tf.expand_dims(prob,1),ref) ,1)\n",
    "\n",
    "        # attention\n",
    "        _, logit = self.pointer(hy,context,Env)\n",
    "        if self.mask_pointer:\n",
    "            logit -= self.BIGNUMBER* Env.mask\n",
    "    \n",
    "        return logit , decoder_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d84caa-1684-424d-8ff1-519176af78ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "args, prt = ParseParams()\n",
    "batch_size = 2\n",
    "nodes = 5\n",
    "cust = nodes-1\n",
    "args['batch_size'] = batch_size\n",
    "args['n_nodes'] = nodes\n",
    "args['n_cust'] = cust\n",
    "clAttentionActor = AttentionVRPActor  #change to use Attention actor\n",
    "decodeStep = RNNDecodeStep(clAttentionActor,\n",
    "                        args,\n",
    "                        args['hidden_dim'],\n",
    "                        use_tanh=args['use_tanh'],\n",
    "                        tanh_exploration=args['tanh_exploration'],\n",
    "                        n_glimpses=args['n_glimpses'],\n",
    "                        mask_glimpses=args['mask_glimpses'],\n",
    "                        mask_pointer=args['mask_pointer'],\n",
    "                        forget_bias=args['forget_bias'],\n",
    "                        rnn_layers=args['rnn_layers'],\n",
    "                        _scope='Actor/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f53361e-c5a6-46af-8d27-f4a566431cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_width = 1\n",
    "embedder_model = FullGraphEmbedding(args['embedding_dim'],args)\n",
    "env = Env( args )\n",
    "env.reset()\n",
    "\n",
    "input_data_norm = env.input_data_norm\n",
    "encoder_emb_inp = embedder_model(input_data_norm)\n",
    "t1 = encoder_emb_inp[:,env.n_nodes-1]\n",
    "t2 = tf.expand_dims(t1, 1)\n",
    "t3 = tf.tile( t2, [1,1,1])\n",
    "decoder_input = tf.tile(tf.expand_dims(encoder_emb_inp[:,env.n_nodes-1], 1),\n",
    "                                [beam_width,1,1])\n",
    "context = tf.tile(encoder_emb_inp,[beam_width,1,1])\n",
    "\n",
    "# decoder_state\n",
    "initial_state = tf.zeros([args['rnn_layers'], 2, args['batch_size']*beam_width, args['hidden_dim']])\n",
    "l = tf.unstack(initial_state, axis=0)  #l is a list of tensors\n",
    "decoder_state = tuple([tf.compat.v1.nn.rnn_cell.LSTMStateTuple(l[idx][0],l[idx][1])\n",
    "          for idx in range(args['rnn_layers'])])\n",
    "\n",
    "#logit, prob, logprob, decoder_state = decodeStep.step(decoder_input,\n",
    "#                                context,\n",
    "#                                env,\n",
    "#                                decoder_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adafb6f-3ac7-4e94-bba0-c7ced2adffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "BatchSequence = tf.expand_dims(tf.cast(tf.range(batch_size*beam_width), tf.int64), 1)\n",
    "# create tensors and lists\n",
    "actions_tmp = []\n",
    "logprobs = []\n",
    "probs = []\n",
    "idxs = []\n",
    "\n",
    "for i in range(args['decode_len']):  # decode_len is 20\n",
    "\n",
    "    logit, prob, logprob, decoder_state = decodeStep.step(decoder_input,\n",
    "                        context,\n",
    "                        env,\n",
    "                        decoder_state)\n",
    "    # idx: [batch_size*beam_width x 1]\n",
    "    beam_parent = None\n",
    "    def my_multinomial():\n",
    "        prob_idx = tf.stop_gradient(prob)\n",
    "        print( ' prob_idx: ', prob_idx.numpy())\n",
    "        prob_idx_cum = tf.cumsum(prob_idx,1)\n",
    "        print( ' prob_idx_cum：', prob_idx_cum.numpy())\n",
    "        rand_uni = tf.tile(tf.random.uniform([batch_size,1]),[1,env.n_nodes])\n",
    "        print( ' rand_uni: ', rand_uni.numpy())\n",
    "        # sorted_ind : [[0,1,2,3..],[0,1,2,3..] , ]\n",
    "        sorted_ind = tf.cast(tf.tile(tf.expand_dims(tf.range(env.n_nodes),0),[batch_size,1]),tf.int64)\n",
    "        print( ' sorted_ind: ', sorted_ind.numpy())\n",
    "        tmp = tf.multiply(tf.cast(tf.greater(prob_idx_cum,rand_uni),tf.int64), sorted_ind)+\\\n",
    "            10000*tf.cast(tf.greater_equal(rand_uni,prob_idx_cum),tf.int64)\n",
    "        print( ' tmp: ', tmp.numpy())\n",
    "        idx = tf.expand_dims(tf.argmin(input=tmp,axis=1),1)\n",
    "        print( ' idx: ', idx.numpy())\n",
    "        return tmp, idx\n",
    "\n",
    "    tmp, idx = my_multinomial()\n",
    "    # check validity of tmp -> True or False -- True mean take a new sample\n",
    "    t1 = tf.reduce_sum(input_tensor=tmp,axis=1)\n",
    "    print( ' t1: ', t1.numpy())\n",
    "    t2 = tf.greater( t1, (10000*env.n_nodes)-1 )\n",
    "    print( ' t2: ', t2.numpy())\n",
    "    tmp_check = tf.cast(tf.reduce_sum(input_tensor=tf.cast(tf.greater(tf.reduce_sum(input_tensor=tmp,axis=1),(10000*env.n_nodes)-1),\n",
    "                                              tf.int32)),tf.bool)\n",
    "    tmp , idx = tf.cond(pred=tmp_check,true_fn=my_multinomial,false_fn=lambda:(tmp,idx))\n",
    "    \n",
    "    state = env.step(idx,beam_parent)\n",
    "    print( ' load status: ', state.load.numpy() )\n",
    "    print( ' demand status: ', state.demand.numpy() )\n",
    "    print( ' satisfied demand: ', state.d_sat.numpy())\n",
    "    print( ' mask: ', state.mask.numpy())\n",
    "    batched_idx = tf.concat([BatchSequence,idx],1)\n",
    "\n",
    "\n",
    "    decoder_input = tf.expand_dims(tf.gather_nd(\n",
    "        tf.tile(encoder_emb_inp,[beam_width,1,1]), batched_idx),1)\n",
    "\n",
    "    logprob = tf.math.log(tf.gather_nd(prob, batched_idx))\n",
    "    probs.append(prob)\n",
    "    idxs.append(idx)\n",
    "    logprobs.append(logprob)\n",
    "\n",
    "    action = tf.gather_nd(tf.tile( env.input_pnt, [beam_width,1,1]), batched_idx )\n",
    "    actions_tmp.append(action)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8b661f-5399-405f-8cb8-b40e3f996592",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
