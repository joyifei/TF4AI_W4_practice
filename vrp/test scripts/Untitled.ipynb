{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "337cd8b3-41fb-42da-b5a5-b4a1a6c49c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "from configs import ParseParams\n",
    "import collections\n",
    "from DataGenerator import DataGenerator\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "class Env(object):\n",
    "    def __init__(self,\n",
    "                 args):\n",
    "        '''\n",
    "        This is the environment for VRP.\n",
    "        Inputs:\n",
    "            args: the parameter dictionary. It should include:\n",
    "                args['n_nodes']: number of nodes in VRP\n",
    "                args['n_custs']: number of customers in VRP\n",
    "                args['input_dim']: dimension of the problem which is 3\n",
    "        '''\n",
    "        self.capacity = args['capacity']\n",
    "        self.n_nodes = args['n_nodes']\n",
    "        self.n_cust = args['n_cust']\n",
    "        self.input_dim = args['input_dim']\n",
    "        \n",
    "        self.data_Gen = DataGenerator(args)\n",
    "        self.input_data = self.data_Gen.get_train_next()\n",
    "        self.input_data = tf.cast( self.input_data, dtype=tf.float32)\n",
    "        #self.input_data = tf.compat.v1.placeholder(tf.float32,\\\n",
    "        #    shape=[None,self.n_nodes,self.input_dim])       # The dimension of the first (None) can be of any size\n",
    "\n",
    "        #self.embeded_data = tf.compat.v1.placeholder(tf.float32,shape=[None,self.n_nodes,30])\n",
    "        #self.input_data_norm = tf.compat.v1.placeholder(tf.float32,\\\n",
    "        #    shape=[None,self.n_nodes,self.input_dim])       # The dimension of the first (None) can be of any size\n",
    "\n",
    "        self.input_pnt = self.input_data[:,:,:(self.input_dim -1)]      # all but demand\n",
    "        self.demand = self.input_data[:,:,-1]\n",
    "        self.batch_size = tf.shape(input=self.input_pnt)[0]\n",
    "        self.args = args\n",
    "    \n",
    "    def initialize_train_step( self ):\n",
    "        self.input_data = self.data_Gen.get_train_next()\n",
    "        self.input_data = tf.cast( self.input_data, dtype=tf.float32)\n",
    "        input_concat = np.concatenate( self.input_data )\n",
    "        before_norm_by_feature = np.reshape(np.transpose(input_concat),(self.args['input_dim'],-1))\n",
    "        norm_by_feature = normalize(before_norm_by_feature, axis=1)\n",
    "        self.input_data_norm = np.reshape(np.transpose(norm_by_feature),(data.shape[0],data.shape[1],data.shape[2]))\n",
    "        self.embeded_data = np.zeros(shape=(self.args['batch_size'],self.args['n_nodes'],self.args['embedding_dim']))\n",
    "   \n",
    "    def reset(self,beam_width=1):\n",
    "        '''\n",
    "        Resets the environment. This environment might be used with different decoders.\n",
    "        In case of using with beam-search decoder, we need to have to increase\n",
    "        the rows of the mask by a factor of beam_width.\n",
    "        '''\n",
    "\n",
    "        # dimensions\n",
    "        self.beam_width = beam_width\n",
    "        self.batch_beam = self.batch_size * beam_width\n",
    "\n",
    "        self.input_pnt = self.input_data[:,:,:2]        # corresponds to all x,y\n",
    "        self.demand = self.input_data[:,:,-1]           # corresponds to all the demand, sixe[batch,nb_nodes]\n",
    "\n",
    "        # modify the self.input_pnt and self.demand for beam search decoder\n",
    "#         self.input_pnt = tf.tile(self.input_pnt, [self.beam_width,1,1])\n",
    "\n",
    "        # demand: [batch_size * beam_width, max_time]\n",
    "        # demand[i] = demand[i+batchsize]\n",
    "        self.demand = tf.tile(self.demand, [self.beam_width,1])\n",
    "\n",
    "        # load: [batch_size * beam_width]\n",
    "        self.load = tf.ones([self.batch_beam])*self.capacity\n",
    "\n",
    "        # create mask\n",
    "        self.mask = tf.zeros([self.batch_size*beam_width,self.n_nodes],\n",
    "                dtype=tf.float32)\n",
    "\n",
    "        # update mask -- mask if customer demand is 0 and depot\n",
    "        self.mask = tf.concat([tf.cast(tf.equal(self.demand,0), tf.float32)[:,:-1],\n",
    "            tf.ones([self.batch_beam,1])],1)\n",
    "\n",
    "        state = State(load=self.load,\n",
    "                    demand = self.demand,\n",
    "                    d_sat = tf.zeros([self.batch_beam,self.n_nodes]),\n",
    "                    mask = self.mask )\n",
    "\n",
    "        return state\n",
    "\n",
    "    def step(self,\n",
    "             idx,\n",
    "             beam_parent=None):\n",
    "        '''\n",
    "        runs one step of the environment and updates demands, loads and masks\n",
    "        '''\n",
    "\n",
    "        # if the environment is used in beam search decoder\n",
    "        if beam_parent is not None:\n",
    "            # BatchBeamSeq: [batch_size*beam_width x 1]\n",
    "            # [0,1,2,3,...,127,0,1,...],\n",
    "            batchBeamSeq = tf.expand_dims(tf.tile(tf.cast(tf.range(self.batch_size), tf.int64),\n",
    "                                                 [self.beam_width]),1)\n",
    "            # batchedBeamIdx:[batch_size*beam_width]\n",
    "            batchedBeamIdx= batchBeamSeq + tf.cast(self.batch_size,tf.int64)*beam_parent\n",
    "            # demand:[batch_size*beam_width x sourceL]\n",
    "            self.demand= tf.gather_nd(self.demand,batchedBeamIdx)\n",
    "            #load:[batch_size*beam_width]\n",
    "            self.load = tf.gather_nd(self.load,batchedBeamIdx)\n",
    "            #MASK:[batch_size*beam_width x sourceL]\n",
    "            self.mask = tf.gather_nd(self.mask,batchedBeamIdx)\n",
    "\n",
    "\n",
    "        BatchSequence = tf.expand_dims(tf.cast(tf.range(self.batch_beam), tf.int64), 1)\n",
    "        batched_idx = tf.concat([BatchSequence,idx],1)\n",
    "\n",
    "        # how much the demand is satisfied\n",
    "        temp = tf.gather_nd( self.demand, batched_idx )\n",
    "        d_sat = tf.minimum( temp, self.load)\n",
    "\n",
    "        # update the demand\n",
    "        t1 = tf.cast(tf.shape(input=self.demand),tf.int64)\n",
    "        print( t1.numpy())\n",
    "        d_scatter = tf.scatter_nd(batched_idx, d_sat, tf.cast(tf.shape(input=self.demand),tf.int64))      # sparse tensor containing d_sat for the interesting idx\n",
    "        print( d_scatter.numpy())\n",
    "        self.demand = tf.subtract(self.demand, d_scatter)\n",
    "        print( self.demand.numpy())\n",
    "        # update load\n",
    "        self.load -= d_sat\n",
    "\n",
    "        # refill the truck -- idx: [10,9,10] -> load_flag: [1 0 1]\n",
    "        t1 = tf.equal( idx, self.n_cust)\n",
    "        t2 = tf.cast(t1, tf.float32)\n",
    "        t3 = tf.squeeze( t2, 1)\n",
    "        #check any of the selected idx is referring to the depot\n",
    "        load_flag = tf.squeeze(tf.cast(tf.equal(idx,self.n_cust),tf.float32),1)\n",
    "        self.load = tf.multiply(self.load,1-load_flag) + load_flag *self.capacity\n",
    "        #for any batch, if it's refilled, then the load reset to capacity\n",
    "        # mask for customers with zero demand\n",
    "        t1 =  tf.equal(self.demand,0)\n",
    "        t2 = tf.cast( t1, tf.float32 )\n",
    "        t3 = t2[:,:-1]\n",
    "        t4 = tf.zeros([self.batch_beam, 1])\n",
    "        t5 = [t3, t4]\n",
    "        t6 = tf.concat(t5, 1)\n",
    "        self.mask = tf.concat([tf.cast(tf.equal(self.demand,0), tf.float32)[:,:-1],\n",
    "                                          tf.zeros([self.batch_beam,1])],1)\n",
    "\n",
    "        # mask if load= 0\n",
    "        # mask if in depot and there is still a demand\n",
    "        t1 = tf.cast(tf.equal(self.load,0),    \n",
    "            tf.float32)     #any load empty?\n",
    "        t2 = tf.expand_dims( t1, 1)   #add back batch dimension\n",
    "        t3 = tf.tile( t2, [1,self.n_cust])\n",
    "        \n",
    "        t4 = tf.reduce_sum(input_tensor=self.demand,axis=1)  #remaining sum  of demand for each element of the batch\n",
    "        t5 = tf.greater( t4, 0)\n",
    "        t6 = tf.cast( t5, tf.float32)   # 1 if there are still demands\n",
    "        \n",
    "        t7 = tf.equal(idx,self.n_cust)\n",
    "        t8 = tf.cast( t7, tf.float32)\n",
    "        t9 = tf.squeeze( t8 )         #refil mask\n",
    "        \n",
    "        t10 = tf.multiply( t6, t9 )\n",
    "        t11 = tf.expand_dims( t10, 1 )\n",
    "        \n",
    "        t12 = tf.concat( [t3, t11],1) #t3 is tensor of [batch, customer], element value 0 means load is not empty yet, 1 mean load is empty, can't satisfy\n",
    "                                      # any more  customer demands, need to go back depot to refill\n",
    "                                      # t11 is tensor of shape [batch, 1], element value 1 means there is still demand for that batch and the truck just goes \n",
    "                                      # back to depot to refil,  next step the truck should go out again next step\n",
    "        print( t12.numpy() )\n",
    "        #in test data, first step, t12 is a [4,11] tensor with all four 0 values,  means there some customers demand satsified, but there is still demand for each case and no tack to depot to refill happened\n",
    "        #  and no demand with 0 amount \n",
    "        self.mask += tf.concat( [tf.tile(tf.expand_dims(tf.cast(tf.equal(self.load,0),\n",
    "            tf.float32),1), [1,self.n_cust]),\n",
    "            tf.expand_dims(tf.multiply(tf.cast(tf.greater(tf.reduce_sum(input_tensor=self.demand,axis=1),0),tf.float32),\n",
    "                             tf.squeeze( tf.cast(tf.equal(idx,self.n_cust),tf.float32))),1)],1)\n",
    "\n",
    "        state = State(load=self.load,\n",
    "                    demand = self.demand,\n",
    "                    d_sat = d_sat,\n",
    "                    mask = self.mask )\n",
    "\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47c975b1-fff3-4c42-b7e9-0743eb55bbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created train iterator.\n",
      "Loading dataset for vrp-size-1000-len-11-test.txt...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method Env.initialize_train_step of <__main__.Env object at 0x000001B8655BC9A0>>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args, prt = ParseParams()\n",
    "batch_size = 4\n",
    "args['batch_size'] = batch_size\n",
    "env = Env( args)\n",
    "env.initialize_train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58edfa5e-b734-400a-8cd2-32fae17ddd68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
