{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91d79465-1181-44f6-8518-2b8a5bea3afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from configs import ParseParams\n",
    "from VRP.vrp_utils import DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81cb7187-9dc9-4502-92c0-1a0800501f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor_net_lr: 0.0001\n",
      "agent_type: attention\n",
      "batch_size: 128\n",
      "beam_width: 5\n",
      "capacity: 20\n",
      "critic_net_lr: 0.0001\n",
      "data_dir: ./data\n",
      "decode_len: 20\n",
      "demand_max: 9\n",
      "disable_tqdm: True\n",
      "dropout: 0.1\n",
      "embedding_dim: 30\n",
      "embedding_graph: 2\n",
      "entropy_coeff: 0.0\n",
      "forget_bias: 1.0\n",
      "gpu: 3\n",
      "hidden_dim: 128\n",
      "infer_type: batch\n",
      "input_dim: 3\n",
      "is_train: True\n",
      "load_path: \n",
      "log_dir: logs/vrp10-2021-09-19_12-14-49\n",
      "log_interval: 200\n",
      "mask_glimpses: True\n",
      "mask_pointer: True\n",
      "max_grad_norm: 2.0\n",
      "min_trucks: False\n",
      "model_dir: logs/vrp10-2021-09-19_12-14-49\\model\n",
      "n_cust: 10\n",
      "n_glimpses: 0\n",
      "n_nodes: 11\n",
      "n_process_blocks: 3\n",
      "n_train: 260000\n",
      "random_seed: 24601\n",
      "rnn_layers: 1\n",
      "save_interval: 1000\n",
      "stdout_print: True\n",
      "tanh_exploration: 10.0\n",
      "task: vrp10\n",
      "task_name: vrp\n",
      "test_interval: 200\n",
      "test_size: 1000\n",
      "ups: False\n",
      "use_tanh: False\n",
      "Created train iterator.\n",
      "Loading dataset for vrp-size-1000-len-11-test.txt...\n"
     ]
    }
   ],
   "source": [
    "args, prt = ParseParams()\n",
    "batch_size = 4\n",
    "args['batch_size'] = batch_size\n",
    "data_Gen = DataGenerator(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d03c624-51b3-41dc-9adb-0b0513944f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env(object):\n",
    "    def __init__(self ):\n",
    "        '''\n",
    "        This is the environment for VRP.\n",
    "        Inputs:\n",
    "            args: the parameter dictionary. It should include:\n",
    "                args['n_nodes']: number of nodes in VRP\n",
    "                args['n_custs']: number of customers in VRP\n",
    "                args['input_dim']: dimension of the problem which is 3\n",
    "        '''\n",
    "        self.capacity = 20\n",
    "        self.n_nodes = 11\n",
    "        self.n_cust = 10\n",
    "        self.input_dim = 3\n",
    "        self.input_data = data_Gen.get_train_next()\n",
    "        self.input_data = tf.cast( self.input_data, dtype=tf.float32)\n",
    "        #self.input_data = tf.compat.v1.placeholder(tf.float32,\\\n",
    "        #    shape=[None,self.n_nodes,self.input_dim])       # The dimension of the first (None) can be of any size\n",
    "\n",
    "        #self.embeded_data = tf.compat.v1.placeholder(tf.float32,shape=[None,self.n_nodes,30])\n",
    "        #self.input_data_norm = tf.compat.v1.placeholder(tf.float32,\\\n",
    "        #    shape=[None,self.n_nodes,self.input_dim])       # The dimension of the first (None) can be of any size\n",
    "\n",
    "        self.input_pnt = self.input_data[:,:,:(self.input_dim -1)]      # all but demand\n",
    "        self.demand = self.input_data[:,:,-1]\n",
    "        self.batch_size = tf.shape(input=self.input_pnt)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b942e1c4-ff18-4410-a47a-4dfec1ba7b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionVRPCritic(object):\n",
    "    \"\"\"A generic attention module for the attention in vrp model\"\"\"\n",
    "    def __init__(self, dim, use_tanh=False, C=10,_name='Attention',_scope=''):\n",
    "\n",
    "        self.use_tanh = use_tanh\n",
    "        self._scope = _scope\n",
    "\n",
    "        with tf.compat.v1.variable_scope(_scope+_name):\n",
    "            # self.v: is a variable with shape [1 x dim]\n",
    "            self.v = tf.compat.v1.get_variable('v',[1,dim],\n",
    "                       initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "            self.v = tf.expand_dims(self.v,2)\n",
    "            \n",
    "        self.emb_d = tf.compat.v1.layers.Conv1D(dim,1,_scope=_scope+_name +'/emb_d') #conv1d\n",
    "        self.project_d = tf.compat.v1.layers.Conv1D(dim,1,_scope=_scope+_name +'/proj_d') #conv1d_1\n",
    "        \n",
    "        self.project_query = tf.compat.v1.layers.Dense(dim,_scope=_scope+_name +'/proj_q') #\n",
    "        self.project_ref = tf.compat.v1.layers.Conv1D(dim,1,_scope=_scope+_name +'/proj_e') #conv1d_2\n",
    "\n",
    "        self.C = C  # tanh exploration parameter\n",
    "        self.tanh = tf.nn.tanh\n",
    "        \n",
    "    def __call__(self, query, ref, env):\n",
    "        \"\"\"\n",
    "        This function gets a query tensor and ref rensor and returns the logit op.\n",
    "        Args: \n",
    "            query: is the hidden state of the decoder at the current\n",
    "                time step. [batch_size x dim]\n",
    "            ref: the set of hidden states from the encoder. \n",
    "                [batch_size x max_time x dim]\n",
    "\n",
    "            env: keeps demand ond load values and help decoding. Also it includes mask.\n",
    "                env.mask: a matrix used for masking the logits and glimpses. It is with shape\n",
    "                         [batch_size x max_time]. Zeros in this matrix means not-masked nodes. Any \n",
    "                         positive number in this mask means that the node cannot be selected as next \n",
    "                         decision point.\n",
    "                env.demands: a list of demands which changes over time.\n",
    "\n",
    "        Returns:\n",
    "            e: convolved ref with shape [batch_size x max_time x dim]\n",
    "            logits: [batch_size x max_time]\n",
    "        \"\"\"\n",
    "        # we need the first demand value for the critic\n",
    "        demand = env.input_data[:,:,-1]\n",
    "        max_time = tf.shape(input=demand)[1]\n",
    "\n",
    "        # embed demand and project it\n",
    "        # emb_d:[batch_size x max_time x dim ]\n",
    "        emb_d = self.emb_d(tf.expand_dims(demand,2))\n",
    "        # d:[batch_size x max_time x dim ]\n",
    "        d = self.project_d(emb_d)\n",
    "\n",
    "\n",
    "        # expanded_q,e: [batch_size x max_time x dim]\n",
    "        e = self.project_ref(ref)\n",
    "        q = self.project_query(query) #[batch_size x dim]\n",
    "        expanded_q = tf.tile(tf.expand_dims(q,1),[1,max_time,1])\n",
    "\n",
    "        # v_view:[batch_size x dim x 1]\n",
    "        v_view = tf.tile( self.v, [tf.shape(input=e)[0],1,1]) \n",
    "        \n",
    "        # u : [batch_size x max_time x dim] * [batch_size x dim x 1] = \n",
    "        #       [batch_size x max_time]\n",
    "        u = tf.squeeze(tf.matmul(self.tanh(expanded_q + e + d), v_view),2)\n",
    "\n",
    "        if self.use_tanh:\n",
    "            logits = self.C * self.tanh(u)\n",
    "        else:\n",
    "            logits = u  \n",
    "\n",
    "        return e, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c750e3-2eb5-46d7-8e38-82ec49eae278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.08945787 -0.1517034  -0.1602616  -0.44746327]\n"
     ]
    }
   ],
   "source": [
    "ref = tf.cast( np.random.randn( args['batch_size'], args['n_nodes'], args['embedding_dim']), tf.float32 )\n",
    "env = Env()\n",
    "with tf.compat.v1.variable_scope(\"Critic\"):\n",
    "                with tf.compat.v1.variable_scope(\"Encoder\"):\n",
    "                    # init states\n",
    "                    initial_state = tf.zeros([args['rnn_layers'], 2, batch_size, args['hidden_dim']])\n",
    "                    l = tf.unstack(initial_state, axis=0)\n",
    "                    rnn_tuple_state = tuple([tf.compat.v1.nn.rnn_cell.LSTMStateTuple(l[idx][0],l[idx][1]) # index + corresponds to coord\n",
    "                              for idx in range(args['rnn_layers'])])\n",
    "\n",
    "                    hy = rnn_tuple_state[0][1]\n",
    "\n",
    "                with tf.compat.v1.variable_scope(\"Process\"):\n",
    "                    for i in range(args['n_process_blocks']):\n",
    "\n",
    "                        process = AttentionVRPCritic(args['hidden_dim'],_name=\"P\"+str(i))\n",
    "                        e,logit = process(hy, ref, env)\n",
    "\n",
    "                        prob = tf.nn.softmax(logit)\n",
    "                        # hy : [batch_size x 1 x sourceL] * [batch_size  x sourceL x hidden_dim]  ->\n",
    "                        #[batch_size x h_dim ]\n",
    "                        hy = tf.squeeze(tf.matmul(tf.expand_dims(prob,1), e ) ,1)\n",
    "\n",
    "                with tf.compat.v1.variable_scope(\"Linear\"):\n",
    "                    v = tf.squeeze(tf.compat.v1.layers.dense(tf.compat.v1.layers.dense(hy,args['hidden_dim']\\\n",
    "                                                               ,tf.nn.relu,name='L1'),1,name='L2'),1)\n",
    "                    print( v.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700480b5-d0b2-47fa-85f6-933069fed3d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
