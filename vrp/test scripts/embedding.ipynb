{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2235ac8-0235-4cdd-959d-591516e6021b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "from typing import List, Optional\n",
    "from typing import Callable, List\n",
    "from env import Env\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from shared.embeddings import Embedding\n",
    "from shared.graph_embedding.useful_files.utils import get_activation\n",
    "from configs import ParseParams\n",
    "from VRP.vrp_utils import DataGenerator\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e854cf8-0a1a-42bf-ab88-df0193583f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(input_tensor, name=None):\n",
    "  \"\"\"Run layer normalization on the last dimension of the tensor.\"\"\"\n",
    "  return tf.keras.layers.LayerNormalization(name=name,axis=-1,epsilon=1e-12,dtype=tf.float32)(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ace44856-ce73-4a7e-8d19-ea94f7acbcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregation_function(aggregation_fun: Optional[str]):\n",
    "    if aggregation_fun in ['sum', 'unsorted_segment_sum']:\n",
    "        return tf.math.unsorted_segment_sum\n",
    "    if aggregation_fun in ['max', 'unsorted_segment_max']:\n",
    "        return tf.math.unsorted_segment_max\n",
    "    if aggregation_fun in ['mean', 'unsorted_segment_mean']:\n",
    "        return tf.math.unsorted_segment_mean\n",
    "    if aggregation_fun in ['sqrt_n', 'unsorted_segment_sqrt_n']:\n",
    "        return tf.math.unsorted_segment_sqrt_n\n",
    "    else:\n",
    "        raise ValueError(\"Unknown aggregation function '%s'!\" % aggregation_fun)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad37aa8b-98cc-43de-b050-84c3eeccdfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_gnn_film_layer(node_embeddings: tf.Tensor,\n",
    "                          adjacency_lists: List[tf.Tensor],\n",
    "                          type_to_num_incoming_edges: tf.Tensor,\n",
    "                          state_dim: Optional[int],\n",
    "                          num_timesteps: int = 1,\n",
    "                          activation_function: Optional[str] = \"ReLU\",\n",
    "                          message_aggregation_function: str = \"sum\",\n",
    "                          normalize_by_num_incoming: bool = False,\n",
    "                          ) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Compute new graph states by neural message passing modulated by the target state.\n",
    "    For this, we assume existing node states h^t_v and a list of per-edge-type adjacency\n",
    "    matrices A_\\ell.\n",
    "\n",
    "    We compute new states as follows:\n",
    "        h^{t+1}_v := \\sum_\\ell\n",
    "                     \\sum_{(u, v) \\in A_\\ell}\n",
    "                        \\sigma(1/c_{v,\\ell} * \\alpha_{\\ell,v} * (W_\\ell * h^t_u) + \\beta_{\\ell,v})\n",
    "        \\alpha_{\\ell,v} := F_{\\ell,\\alpha} * h^t_v\n",
    "        \\beta_{\\ell,v} := F_{\\ell,\\beta} * h^t_v\n",
    "        c_{\\v,\\ell} is usually 1 (but could also be the number of incoming edges).\n",
    "    The learnable parameters of this are the W_\\ell, F_{\\ell,\\alpha}, F_{\\ell,\\beta} \\in R^{D, D}.\n",
    "\n",
    "    We use the following abbreviations in shape descriptions:\n",
    "    * V: number of nodes\n",
    "    * D: state dimension\n",
    "    * L: number of different edge types\n",
    "    * E: number of edges of a given edge type\n",
    "\n",
    "    Arguments:\n",
    "        node_embeddings: float32 tensor of shape [V, D], the original representation of\n",
    "            each node in the graph.\n",
    "        adjacency_lists: List of L adjacency lists, represented as int32 tensors of shape\n",
    "            [E, 2]. Concretely, adjacency_lists[l][k,:] == [v, u] means that the k-th edge\n",
    "            of type l connects node v to node u.\n",
    "        type_to_num_incoming_edges: float32 tensor of shape [L, V] representing the number\n",
    "            of incoming edges of a given type. Concretely, type_to_num_incoming_edges[l, v]\n",
    "            is the number of edge of type l connecting to node v.\n",
    "        state_dim: Optional size of output dimension of the GNN layer. If not set, defaults\n",
    "            to D, the dimensionality of the input. If different from the input dimension,\n",
    "            parameter num_timesteps has to be 1.\n",
    "        num_timesteps: Number of repeated applications of this message passing layer.\n",
    "        activation_function: Type of activation function used.\n",
    "        message_aggregation_function: Type of aggregation function used for messages.\n",
    "        normalize_by_num_incoming: Flag indicating if messages should be scaled by 1/(number\n",
    "            of incoming edges).\n",
    "\n",
    "    Returns:\n",
    "        float32 tensor of shape [V, state_dim]\n",
    "    \"\"\"\n",
    "    num_nodes = tf.shape(input=node_embeddings, out_type=tf.int32)[0]\n",
    "    if state_dim is None:\n",
    "        state_dim = tf.shape(input=node_embeddings, out_type=tf.int32)[1]\n",
    "\n",
    "    # === Prepare things we need across all timesteps:\n",
    "    activation_fn = get_activation(activation_function)\n",
    "    message_aggregation_fn = get_aggregation_function(message_aggregation_function)\n",
    "    edge_type_to_message_transformation_layers = []  # Layers to compute the message from a source state\n",
    "    edge_type_to_film_computation_layers = []  # Layers to compute the \\beta/\\gamma weights for FiLM\n",
    "    edge_type_to_message_targets = []  # List of tensors of message targets\n",
    "    for edge_type_idx, adjacency_list_for_edge_type in enumerate(adjacency_lists):\n",
    "        edge_type_to_message_transformation_layers.append(\n",
    "            tf.keras.layers.Dense(units=state_dim,\n",
    "                                  use_bias=False,\n",
    "                                  activation=None,  # Activation only after FiLM modulation\n",
    "                                  name=\"Edge_%i_Weight\" % edge_type_idx))\n",
    "        edge_type_to_film_computation_layers.append(\n",
    "            tf.keras.layers.Dense(units=2 * state_dim,  # Computes \\gamma, \\beta in one go\n",
    "                                  use_bias=False,\n",
    "                                  activation=None,\n",
    "                                  name=\"Edge_%i_FiLM_Computations\" % edge_type_idx))\n",
    "        edge_type_to_message_targets.append(tf.cast(adjacency_list_for_edge_type[:, 1],dtype=tf.int32))\n",
    "\n",
    "    # Let M be the number of messages (sum of all E):\n",
    "    message_targets = tf.concat(edge_type_to_message_targets, axis=0)  # Shape [M]\n",
    "\n",
    "    cur_node_states = node_embeddings\n",
    "    for _ in range(num_timesteps):\n",
    "        messages_per_type = []  # list of tensors of messages of shape [E, D]\n",
    "        # Collect incoming messages per edge type\n",
    "        for edge_type_idx, adjacency_list_for_edge_type in enumerate(adjacency_lists):\n",
    "            edge_sources = tf.cast(adjacency_list_for_edge_type[:, 0],dtype=tf.int32)\n",
    "            edge_targets = tf.cast(adjacency_list_for_edge_type[:, 1],dtype=tf.int32)\n",
    "            edge_source_states = \\\n",
    "                tf.nn.embedding_lookup(params=cur_node_states,\n",
    "                                       ids=edge_sources)  # Shape [E, D]\n",
    "            #embedding_lookup:  if params is a tensor like [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]], \n",
    "            #and ids is: [0, 3, 4],  then return value would be [[1, 2], [7, 8], [9, 10]]\n",
    "            messages = edge_type_to_message_transformation_layers[edge_type_idx](edge_source_states)  # Shape [E, D]\n",
    "            \n",
    "            print( type_to_num_incoming_edges[edge_type_idx, :].numpy() )\n",
    "            print( edge_targets.numpy() )\n",
    "            if normalize_by_num_incoming:\n",
    "                per_message_num_incoming_edges = \\\n",
    "                    tf.nn.embedding_lookup(params=type_to_num_incoming_edges[edge_type_idx, :],\n",
    "                                           ids=edge_targets)  # Shape [E, H]\n",
    "\n",
    "                messages = tf.expand_dims(1.0 / (per_message_num_incoming_edges + SMALL_NUMBER), axis=-1) * messages\n",
    "\n",
    "            film_weights = edge_type_to_film_computation_layers[edge_type_idx](cur_node_states)\n",
    "            per_message_film_weights = \\\n",
    "                tf.nn.embedding_lookup(params=film_weights, ids=edge_targets)\n",
    "            per_message_film_gamma_weights = per_message_film_weights[:, :state_dim]  # Shape [E, D]\n",
    "            per_message_film_beta_weights = per_message_film_weights[:, state_dim:]  # Shape [E, D]\n",
    "\n",
    "            modulated_messages = per_message_film_gamma_weights * messages + per_message_film_beta_weights\n",
    "            messages_per_type.append(modulated_messages)\n",
    "\n",
    "        all_messages = tf.concat(messages_per_type, axis=0)  # Shape [M, D]\n",
    "        all_messages = activation_fn(all_messages)  # Shape [M, D]\n",
    "        aggregated_messages = \\\n",
    "            message_aggregation_fn(data=all_messages,\n",
    "                                   segment_ids=message_targets,\n",
    "                                   num_segments=num_nodes)  # Shape [V, D]\n",
    "        new_node_states = aggregated_messages\n",
    "        # new_node_states = activation_fn(new_node_states)\n",
    "\n",
    "        cur_node_states = layer_norm(new_node_states)\n",
    "\n",
    "    return cur_node_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9f14fb2-bf58-4e88-b1a8-e80e776dd569",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullGraphEmbedding(Embedding):\n",
    "    \"\"\"\n",
    "    Implements a graph embedding, not test\n",
    "    \"\"\"\n",
    "    def __init__(self,embedding_dim,args):\n",
    "        assert args['embedding_dim'] == 30, args['embedding_dim']\n",
    "        super(FullGraphEmbedding,self).__init__('full_graph',embedding_dim)\n",
    "\n",
    "        self.nb_feat = args['input_dim']\n",
    "        self.n_nodes = args['n_nodes']\n",
    "\n",
    "        self._scale = [5,12,25,50,100]\n",
    "        self._scale = [i * np.sqrt(2)/100 for i in self._scale]     # rescale to the square\n",
    "\n",
    "        self.drop_out = tf.Variable( 1.0, dtype=tf.float32)\n",
    "        #self.drop_out = tf.compat.v1.placeholder(tf.float32,name='embedder_graph_dropout')\n",
    "        self.params = {\n",
    "            'graph_num_layers': 8,\n",
    "            'graph_num_timesteps_per_layer': 3,\n",
    "\n",
    "            'graph_layer_input_dropout_keep_prob': 0.8,\n",
    "            'graph_dense_between_every_num_gnn_layers': 1,\n",
    "            'graph_model_activation_function': 'tanh',\n",
    "            'graph_residual_connection_every_num_layers': 1,\n",
    "            'graph_inter_layer_norm': False,\n",
    "            \"hidden_size\": 30,\n",
    "            \"graph_activation_function\": \"ReLU\",\n",
    "            \"message_aggregation_function\": \"sum\",\n",
    "            \"normalize_messages_by_num_incoming\": True\n",
    "            }\n",
    "\n",
    "    def _propagate_graph_model(self,initial_node_features, incoming_edge, list_pair_adjancy):\n",
    "        \"\"\"\n",
    "        Build the propagation model via graph\n",
    "        :param initial_node_features:\n",
    "        :param incoming_edge:\n",
    "        :param list_pair_adjancy:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        h_dim= self.params['hidden_size']\n",
    "        activation_fn = get_activation(self.params['graph_model_activation_function'])\n",
    "\n",
    "        projected_node_features = tf.keras.layers.Dense(units=h_dim,\n",
    "                                      use_bias=False,\n",
    "                                      activation=activation_fn,\n",
    "                                      )(initial_node_features)\n",
    "\n",
    "        cur_node_representations = projected_node_features\n",
    "        last_residual_representations = tf.zeros_like(cur_node_representations)\n",
    "        for layer_idx in range(self.params['graph_num_layers']):\n",
    "            # with tf.variable_scope('gnn_layer_%i' % layer_idx):\n",
    "            cur_node_representations = \\\n",
    "                tf.nn.dropout(cur_node_representations, rate= 1- self.drop_out)\n",
    "            if layer_idx % self.params['graph_residual_connection_every_num_layers'] == 0:\n",
    "                t = cur_node_representations\n",
    "                if layer_idx > 0:\n",
    "                    cur_node_representations += last_residual_representations\n",
    "                    cur_node_representations /= 2\n",
    "                last_residual_representations = t\n",
    "            cur_node_representations = \\\n",
    "                self._apply_gnn_layer(cur_node_representations,list_pair_adjancy,incoming_edge,self.params['graph_num_timesteps_per_layer'])\n",
    "            if self.params['graph_inter_layer_norm']:\n",
    "                cur_node_representations = tf.contrib.layers.layer_norm(cur_node_representations)\n",
    "            if layer_idx % self.params['graph_dense_between_every_num_gnn_layers'] == 0:\n",
    "                cur_node_representations = \\\n",
    "                    tf.keras.layers.Dense(units=h_dim,\n",
    "                                          use_bias=False,\n",
    "                                          activation=activation_fn,\n",
    "                                          name=\"Dense\",\n",
    "                                          )(cur_node_representations)\n",
    "\n",
    "        return cur_node_representations\n",
    "\n",
    "\n",
    "    def _apply_gnn_layer(self,node_representations,adjacency_lists,type_to_num_incoming_edges,num_timesteps):\n",
    "        \"\"\"\n",
    "        Apply the actual gnn layer\n",
    "        \"\"\"\n",
    "        return sparse_gnn_film_layer(\n",
    "            node_embeddings=node_representations,\n",
    "            adjacency_lists=adjacency_lists,\n",
    "            type_to_num_incoming_edges=type_to_num_incoming_edges,\n",
    "            state_dim=self.params['hidden_size'],\n",
    "            num_timesteps=num_timesteps,\n",
    "            activation_function=self.params['graph_activation_function'],\n",
    "            message_aggregation_function=self.params['message_aggregation_function'],\n",
    "            normalize_by_num_incoming=self.params[\"normalize_messages_by_num_incoming\"])\n",
    "\n",
    "\n",
    "    def _prepare_input_data(self, input_tf):\n",
    "        \"\"\"\n",
    "        Prepare the input data so that they are at the right size\n",
    "        :param input_tf:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        #shape of input_tf is [None, 11, 3] which mean undetermined batches, 11 nodes,  andd 3 columne for each node to list the x, y coordinate and demand qty\n",
    "        batch_features = tf.reshape(input_tf,[-1,self.nb_feat])\n",
    "        #batch features are put the nodes infor together,  into shape [None, 3]\n",
    "        input_dist = input_tf[:,:,:2]\n",
    "        square_input = tf.reduce_sum(input_tensor=tf.square(input_dist), axis=2)\n",
    "        row = tf.reshape(square_input, [-1,self.n_nodes,1])\n",
    "        col= tf.reshape(square_input,[-1,1,self.n_nodes])\n",
    "        print( 'row: ', row.numpy())\n",
    "        print( 'col: ', col.numpy())\n",
    "        t1 = 2 * tf.matmul(input_dist,input_dist,False,True)\n",
    "        dist_matrix = tf.sqrt(tf.maximum(row - 2 * tf.matmul(input_dist,input_dist,False,True) + col,0.0))\n",
    "        #shape of dist_matrix would be [?, self.n_nodes, self.n_nodes]\n",
    "        # value is the distance between nodes,  coordinate of node Ni is (Xi1, Xi2), and node Nj is (Xj1, Xj2)\n",
    "        #then the value in the maxtrix for position[ ?, i, j] would be sqrt( (Xi1 - Xj1 ) ^2 + (Xi2 - Xj2)^2 )\n",
    "        # example dist_matrix: \n",
    "        # dist_matrix: \n",
    "        # tf.Tensor(\n",
    "        #[[[ 0.         4.2426405  8.485281  12.7279215]\n",
    "        #  [ 4.2426405  0.         4.2426405  8.485281 ]\n",
    "        #  [ 8.485281   4.2426405  0.         4.2426405]\n",
    "        #  [12.7279215  8.485281   4.2426405  0.       ]]\n",
    "        # [[ 0.         4.2426405  8.485281  12.7279215]\n",
    "        #  [ 4.2426405  0.         4.2426405  8.485281 ]\n",
    "        #  [ 8.485281   4.2426405  0.         4.2426405]\n",
    "        #  [12.7279215  8.485281   4.2426405  0.       ]]], shape=(2, 4, 4), \n",
    "        list_num_incoming_ege = []\n",
    "        list_pair_edge = []\n",
    "        # not_masked is a [?, self.n_nodes, self.n_nodes] shape boolean tensor, all intial values are true\n",
    "        not_masked = tf.ones_like(dist_matrix, dtype=tf.bool)\n",
    "        temp = tf.zeros_like(not_masked[0,:,:])  #temp will follow no_masked's dtype,  if it's bool,  it will be bool too\n",
    "        #so set_diag will set the diagnal values to zeros, like:\n",
    "        #  [ [0, 1, 1, 1],\n",
    "        #    [1, 0, 1, 1],\n",
    "        #    [1, 1, 0, 1],\n",
    "        print( not_masked.shape)\n",
    "        print( tf.zeros_like(not_masked[0,:,:]).shape )\n",
    "        print( tf.zeros_like(not_masked[:,:,0]).numpy() )\n",
    "        print( tf.zeros_like(not_masked[:,:,0]).shape )\n",
    "        not_masked = tf.linalg.set_diag(not_masked,tf.zeros_like(not_masked[:,:,0])) #linalg_.set_diag won't change not_masked here,  shall change it to not_masked = tf.linalg.set_diag?\n",
    "        \n",
    "        print( not_masked.numpy())\n",
    "        for i in range(len(self._scale)):\n",
    "            true_for_edge = tf.less_equal(dist_matrix,self._scale[i])\n",
    "            true_for_edge = tf.logical_and(not_masked,true_for_edge)\n",
    "            # continue above example , \n",
    "            # true for edge less than 5.0: \n",
    "            #tf.Tensor(\n",
    "            #[[[ True,  True, False],\n",
    "            #  [ False,  True,  True]],\n",
    "            # [[ True,  True, False],\n",
    "            # [ True,  False,  True]]], shape=(2, 2, 3), dtype=bool)\n",
    "            \n",
    "            # all values less than self._scale[i] are true,  and diagnal values are false\n",
    "            # same shape as dist_matrix [?,n,n]\n",
    "            print( true_for_edge.numpy())\n",
    "            indices = tf.cast(tf.compat.v1.where(true_for_edge),dtype=tf.int32)\n",
    "            print( indices.numpy() )\n",
    "            #indices of  coordinates of all the edges in dist_matrix with value less than self._scale[i]\n",
    "            # indices is like:  [[0, 0, 0],\n",
    "            #[0, 0, 1],\n",
    "            #[0, 1, 1],\n",
    "            #[0, 1, 2],\n",
    "            #[1, 0, 0],\n",
    "            #[1, 0, 1],\n",
    "            #[1, 1, 0],\n",
    "            #[1, 1, 2]]\n",
    "            #  notice that the batch dimension is removed,  the following steps will add offset for batch to the index\n",
    "            offset = self.n_nodes * indices[:,0]    # indices' shape is [8,3], so this line will get all batch value\n",
    "            print( offset.numpy() )\n",
    "             # offset of temp would be [0,0,0,0,11,11,11,11]\n",
    "            offset = tf.expand_dims(offset,axis=1)\n",
    "            print( offset.numpy() )\n",
    "            #after expending become: [[0],[0],[0],[0],[11],[11], [11],[11]]\n",
    "            offset = tf.tile(offset,[1,2])\n",
    "            print( offset.numpy() )\n",
    "            #after tiline become:  [[0,0], [0,0], [11,11],[22,22],[22,22],[22,22]]\n",
    "            #indices[:,1:3] is the index of last two column,  so really the edge (from node to 'to node')\n",
    "            true_indices_nodes = offset + indices[:,1:3]\n",
    "            # so now actually true_indices_nodes is like embedding the batching dimension into the edge columns\n",
    "            #[[ 0,  0],\n",
    "           #[ 0,  1],\n",
    "           #[ 1,  1],\n",
    "           #[ 1,  2],\n",
    "           #[11, 11],\n",
    "           #[11, 12],\n",
    "           #[12, 11],\n",
    "           #[12, 13]]\n",
    "            list_pair_edge.append(true_indices_nodes)\n",
    "\n",
    "            num_incoming = tf.reduce_sum(input_tensor=tf.cast(true_for_edge,dtype=tf.int32), axis=1)\n",
    "            # continue examples， num incoming, incoming edges to a node which is less than scale[i]\n",
    "            #tf.Tensor(\n",
    "            #[[1, 2, 1],\n",
    "            #[2, 1, 1]], shape=(2, 3), dtype=int32)\n",
    "            \n",
    "            num_incoming = tf.squeeze(tf.reshape(num_incoming,[1,-1]),0)\n",
    "            # reshaped: tf.Tensor([[1,2,1,2,1,1]], shape=(1, 6), dtype=int32)\n",
    "            # squeezed: tf.Tensor([1,2,1,2,1,1], shape=(6,), dtype=int32)  \n",
    "            list_num_incoming_ege.append(tf.cast(num_incoming,dtype=tf.float32))\n",
    "            # list_num_incoming_ege is a list of 5 tensor\n",
    "            # update the mask\n",
    "            not_masked = tf.logical_and(not_masked,tf.logical_not(true_for_edge)) # we update the mask. The only one not masked are the one wich\n",
    "                                                                                    # were not and did not belong to the edge type\n",
    "            print( not_masked.numpy())\n",
    "        final_incoming_edge = tf.stack(list_num_incoming_ege)   #list_num_incoming_ege is a list of 5 tensors, tensor shape for examples are all (6,),\n",
    "                                                                #  after stack, the will get a tensor with shape (5,6)\n",
    "\n",
    "        print( final_incoming_edge.numpy() )\n",
    "        return batch_features, final_incoming_edge, list_pair_edge\n",
    "\n",
    "    def __call__(self, input_tf):\n",
    "        \"\"\"\n",
    "        return the node embedding\n",
    "        :param input_tf: the tensor corresponding to the embedding\n",
    "        :return: a tensor\n",
    "        \"\"\"\n",
    "        time_init = time.time()\n",
    "        initial_node_features, incoming_edge, list_pair_adjancy = self._prepare_input_data(input_tf)\n",
    "\n",
    "        final_node_representations = self._propagate_graph_model(initial_node_features,incoming_edge,list_pair_adjancy)\n",
    "        final_node_representations = tf.reshape(final_node_representations,[-1,self.n_nodes,self.embedding_dim])\n",
    "\n",
    "        self.total_time += time.time() - time_init\n",
    "\n",
    "        return final_node_representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5a7e4f3-c950-45ee-977b-06c8afd0248e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created train iterator.\n",
      "Loading dataset for vrp-size-1000-len-4-test.txt...\n",
      "Created train iterator.\n",
      "Loading dataset for vrp-size-1000-len-4-test.txt...\n"
     ]
    }
   ],
   "source": [
    "args, prt = ParseParams()\n",
    "batch_size = 2\n",
    "args['batch_size'] = batch_size\n",
    "args['n_nodes'] = 4\n",
    "args['n_cust'] = 3\n",
    "\n",
    "BIG_NUMBER = 1e7\n",
    "SMALL_NUMBER = 1e-7\n",
    "data_Gen = DataGenerator(args)\n",
    "embedder_model = FullGraphEmbedding(args['embedding_dim'],args)\n",
    "env = Env( args )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e9ae51-7a9e-404d-9719-b55d97816b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row:  [[[0.39102334]\n",
      "  [0.24539869]\n",
      "  [0.30723548]\n",
      "  [0.40662676]]\n",
      "\n",
      " [[0.10051714]\n",
      "  [0.0231197 ]\n",
      "  [0.08312111]\n",
      "  [0.44295782]]]\n",
      "col:  [[[0.39102334 0.24539869 0.30723548 0.40662676]]\n",
      "\n",
      " [[0.10051714 0.0231197  0.08312111 0.44295782]]]\n",
      "(2, 4, 4)\n",
      "(4, 4)\n",
      "[[False False False False]\n",
      " [False False False False]]\n",
      "(2, 4)\n",
      "[[[False  True  True  True]\n",
      "  [ True False  True  True]\n",
      "  [ True  True False  True]\n",
      "  [ True  True  True False]]\n",
      "\n",
      " [[False  True  True  True]\n",
      "  [ True False  True  True]\n",
      "  [ True  True False  True]\n",
      "  [ True  True  True False]]]\n",
      "[[[False False False False]\n",
      "  [False False False False]\n",
      "  [False False False False]\n",
      "  [False False False False]]\n",
      "\n",
      " [[False False False False]\n",
      "  [False False False False]\n",
      "  [False False False False]\n",
      "  [False False False False]]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[[False  True  True  True]\n",
      "  [ True False  True  True]\n",
      "  [ True  True False  True]\n",
      "  [ True  True  True False]]\n",
      "\n",
      " [[False  True  True  True]\n",
      "  [ True False  True  True]\n",
      "  [ True  True False  True]\n",
      "  [ True  True  True False]]]\n",
      "[[[False False False False]\n",
      "  [False False False False]\n",
      "  [False False False False]\n",
      "  [False False False False]]\n",
      "\n",
      " [[False  True  True False]\n",
      "  [ True False  True False]\n",
      "  [ True  True False False]\n",
      "  [False False False False]]]\n",
      "[[1 0 1]\n",
      " [1 0 2]\n",
      " [1 1 0]\n",
      " [1 1 2]\n",
      " [1 2 0]\n",
      " [1 2 1]]\n",
      "[4 4 4 4 4 4]\n",
      "[[4]\n",
      " [4]\n",
      " [4]\n",
      " [4]\n",
      " [4]\n",
      " [4]]\n",
      "[[4 4]\n",
      " [4 4]\n",
      " [4 4]\n",
      " [4 4]\n",
      " [4 4]\n",
      " [4 4]]\n",
      "[[[False  True  True  True]\n",
      "  [ True False  True  True]\n",
      "  [ True  True False  True]\n",
      "  [ True  True  True False]]\n",
      "\n",
      " [[False False False  True]\n",
      "  [False False False  True]\n",
      "  [False False False  True]\n",
      "  [ True  True  True False]]]\n",
      "[[[False False  True  True]\n",
      "  [False False False  True]\n",
      "  [ True False False False]\n",
      "  [ True  True False False]]\n",
      "\n",
      " [[False False False False]\n",
      "  [False False False False]\n",
      "  [False False False False]\n",
      "  [False False False False]]]\n",
      "[[0 0 2]\n",
      " [0 0 3]\n",
      " [0 1 3]\n",
      " [0 2 0]\n",
      " [0 3 0]\n",
      " [0 3 1]]\n",
      "[0 0 0 0 0 0]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n",
      "[[[False  True False False]\n",
      "  [ True False  True False]\n",
      "  [False  True False  True]\n",
      "  [False False  True False]]\n",
      "\n",
      " [[False False False  True]\n",
      "  [False False False  True]\n",
      "  [False False False  True]\n",
      "  [ True  True  True False]]]\n",
      "[[[False  True False False]\n",
      "  [ True False  True False]\n",
      "  [False  True False  True]\n",
      "  [False False  True False]]\n",
      "\n",
      " [[False False False  True]\n",
      "  [False False False  True]\n",
      "  [False False False  True]\n",
      "  [ True  True  True False]]]\n",
      "[[0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 2]\n",
      " [0 2 1]\n",
      " [0 2 3]\n",
      " [0 3 2]\n",
      " [1 0 3]\n",
      " [1 1 3]\n",
      " [1 2 3]\n",
      " [1 3 0]\n",
      " [1 3 1]\n",
      " [1 3 2]]\n",
      "[0 0 0 0 0 0 4 4 4 4 4 4]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [4]\n",
      " [4]\n",
      " [4]\n",
      " [4]\n",
      " [4]\n",
      " [4]]\n",
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [4 4]\n",
      " [4 4]\n",
      " [4 4]\n",
      " [4 4]\n",
      " [4 4]\n",
      " [4 4]]\n",
      "[[[False False False False]\n",
      "  [False False False False]\n",
      "  [False False False False]\n",
      "  [False False False False]]\n",
      "\n",
      " [[False False False False]\n",
      "  [False False False False]\n",
      "  [False False False False]\n",
      "  [False False False False]]]\n",
      "[[[False False False False]\n",
      "  [False False False False]\n",
      "  [False False False False]\n",
      "  [False False False False]]\n",
      "\n",
      " [[False False False False]\n",
      "  [False False False False]\n",
      "  [False False False False]\n",
      "  [False False False False]]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[[False False False False]\n",
      "  [False False False False]\n",
      "  [False False False False]\n",
      "  [False False False False]]\n",
      "\n",
      " [[False False False False]\n",
      "  [False False False False]\n",
      "  [False False False False]\n",
      "  [False False False False]]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 2. 2. 2. 0.]\n",
      " [2. 1. 1. 2. 0. 0. 0. 0.]\n",
      " [1. 2. 2. 1. 1. 1. 1. 3.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[]\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "[0. 0. 0. 0. 2. 2. 2. 0.]\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "[5 6 4 6 4 5]\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "[2. 1. 1. 2. 0. 0. 0. 0.]\n",
      "[2 3 3 0 0 1]\n",
      "[1. 2. 2. 1. 1. 1. 1. 3.]\n",
      "[1 0 2 1 3 2 7 7 7 4 5 6]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[]\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[]\n",
      "[0. 0. 0. 0. 2. 2. 2. 0.]\n",
      "[5 6 4 6 4 5]\n",
      "[2. 1. 1. 2. 0. 0. 0. 0.]\n",
      "[2 3 3 0 0 1]\n",
      "[1. 2. 2. 1. 1. 1. 1. 3.]\n",
      "[1 0 2 1 3 2 7 7 7 4 5 6]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[]\n",
      "[0. 0. 0. 0. 2. 2. 2. 0.]\n",
      "[5 6 4 6 4 5]\n",
      "[2. 1. 1. 2. 0. 0. 0. 0.]\n",
      "[2 3 3 0 0 1]\n",
      "[1. 2. 2. 1. 1. 1. 1. 3.]\n",
      "[1 0 2 1 3 2 7 7 7 4 5 6]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[]\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n"
     ]
    }
   ],
   "source": [
    "embedder_model(env.input_data_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ebd7d2-d464-4dc6-9b66-2ac19d238a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
